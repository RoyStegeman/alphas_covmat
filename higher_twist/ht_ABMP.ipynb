{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "226dc610",
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "from validphys.api import API\n",
    "import scipy as sp\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "92f93410",
   "metadata": {},
   "outputs": [],
   "source": [
    "fitname = \"240529-01-fABMP22R-CT\"\n",
    "thcovmat_dict = API.fit(fit=fitname).as_input()[\"theorycovmatconfig\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d7dae95b",
   "metadata": {},
   "outputs": [],
   "source": [
    "H2_coeff_list = thcovmat_dict[\"H2_list\"]\n",
    "HL_coeff_list = thcovmat_dict[\"HL_list\"]\n",
    "\n",
    "\n",
    "# dict used to produce theory predictions to construct the theory covmat as well as to produce\n",
    "# theory predictions from the fit performed using the ht covmat (i.e. the predicitons that should\n",
    "# be compared to data)\n",
    "common_dict = dict(\n",
    "    dataset_inputs={\"from_\": \"fit\"},\n",
    "    fit=fitname,\n",
    "    fits=[fitname],\n",
    "    use_cuts=\"fromfit\",\n",
    "    metadata_group=\"nnpdf31_process\",\n",
    "    theory={\"from_\": \"fit\"},\n",
    "    theoryid={\"from_\": \"theory\"},\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "08d0d9a3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LHAPDF 6.5.4 loading /opt/homebrew/Caskroom/miniconda/base/envs/nnpdf/share/LHAPDF/210619-n3fit-001/210619-n3fit-001_0000.dat\n",
      "210619-n3fit-001 PDF set, member #0, version 1\n",
      "LHAPDF 6.5.4 loading all 101 PDFs in set 210619-n3fit-001\n",
      "210619-n3fit-001, version 1; 101 PDF members\n"
     ]
    }
   ],
   "source": [
    "# collect the information (predictions + kinematics) needed for the computation of the HT covmat\n",
    "\n",
    "# Calculate theory predictions of the input PDF\n",
    "S_dict = dict(\n",
    "    theorycovmatconfig={\"from_\": \"fit\"},\n",
    "    pdf={\"from_\": \"theorycovmatconfig\"},\n",
    "    use_t0=True,\n",
    "    datacuts={\"from_\": \"fit\"},\n",
    "    t0pdfset={\"from_\": \"datacuts\"},\n",
    ")\n",
    "preds_ht_cov_construction = API.group_result_central_table_no_table(**(S_dict | common_dict))\n",
    "preds_ht = pd.DataFrame(preds_ht_cov_construction['theory_central'])\n",
    "\n",
    "# collect the corresponding kinemacs\n",
    "process_info = API.combine_by_type_ht(**(S_dict | common_dict))\n",
    "N_full_data = np.sum([i for i in process_info.sizes.values()])\n",
    "kinematics_DIS = np.concatenate([v for v in [process_info.data[\"DIS NC\"], process_info.data[\"DIS CC\"]]]).T\n",
    "# TO CHECK: IS preds[][1] THE THEORY PREDICTION?\n",
    "preds_DIS = np.concatenate([v for v in [process_info.preds[\"DIS NC\"][1], process_info.preds[\"DIS CC\"][1]]]).T\n",
    "xvals_DIS = kinematics_DIS[0]\n",
    "q2vals_DIS = kinematics_DIS[1]\n",
    "yvals_DIS = kinematics_DIS[2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "9c21605b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LHAPDF 6.5.4 loading all 451 PDFs in set 240529-01-fABMP22R-CT\n",
      "240529-01-fABMP22R-CT, version 1; 451 PDF members\n"
     ]
    }
   ],
   "source": [
    "# Calculate theory predictions of the fit with ht covmat - this will be compared to data\n",
    "preds = API.group_result_table_no_table(pdf={\"from_\": \"fit\"}, **common_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "bc820c3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# compute the matrix X encoding the PDF uncertainties of the predictions\n",
    "preds_onlyreplicas = preds.iloc[:, 2:].to_numpy()\n",
    "mean_prediction = np.mean(preds_onlyreplicas, axis=1)\n",
    "\n",
    "X = np.zeros((preds.shape[0], preds.shape[0]))\n",
    "for i in range(preds_onlyreplicas.shape[1]):\n",
    "    X += np.outer(\n",
    "        (preds_onlyreplicas[:, i] - mean_prediction), (preds_onlyreplicas[:, i] - mean_prediction)\n",
    "    )\n",
    "X *= 1 / preds_onlyreplicas.shape[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "5584ad71",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_normalisation_by_experiment(experiment_name, x, y, Q2):\n",
    "    N_2 = np.zeros(shape=y.shape)\n",
    "    N_L = np.zeros(shape=y.shape)\n",
    "\n",
    "    if \"HERA_NC\" in experiment_name or \"HERA_CC\" in experiment_name or \"NMC\" in experiment_name:\n",
    "        yp = 1 + np.power(1 - y, 2)\n",
    "        yL = np.power(y, 2)\n",
    "\n",
    "        if \"HERA_NC\" in experiment_name or \"NMC\" in experiment_name:\n",
    "            N_2 = 1\n",
    "            N_L = - yL / yp\n",
    "\n",
    "        elif \"HERA_CC\" in experiment_name:\n",
    "            N_2 = 1 / 4 * yp\n",
    "            N_L = - N_2 * yL / yp\n",
    "\n",
    "    if \"CHORUS_CC\" in experiment_name:\n",
    "        yL = np.power(y, 2)\n",
    "        Gf = 1.1663787e-05\n",
    "        Mh = 0.938\n",
    "        MW2 = 80.398 ** 2\n",
    "        yp = 1 + np.power(1 - y, 2) - 2 * np.power(x * y * Mh, 2) / Q2\n",
    "        N_2 = Gf**2 * Mh * yp / ( 2 * np.pi * np.power( 1 + Q2 / MW2, 2) )\n",
    "        N_L = - N_2 * yL / yp\n",
    "\n",
    "    return N_2, N_L"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "76acc02c",
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.options.mode.chained_assignment = None\n",
    "from scipy import interpolate as scint\n",
    "from collections import defaultdict, namedtuple\n",
    "\n",
    "def wrapper_to_splines(i):\n",
    "    if not thcovmat_dict[\"reverse\"]:\n",
    "        shifted_H2_list = [0 for k in range(len(x_abmp))]\n",
    "        shifted_HL_list = [0 for k in range(len(x_abmp))]\n",
    "        shifted_H2_list[i] = H2_coeff_list[i]\n",
    "        shifted_HL_list[i] = HL_coeff_list[i]\n",
    "    else:\n",
    "        shifted_H2_list = H2_coeff_list.copy()\n",
    "        shifted_HL_list = HL_coeff_list.copy()\n",
    "        shifted_H2_list[i] = 0\n",
    "        shifted_HL_list[i] = 0\n",
    "\n",
    "    H_2 = scint.CubicSpline(x_abmp, shifted_H2_list)\n",
    "    H_L = scint.CubicSpline(x_abmp, shifted_HL_list)\n",
    "    H_2 = np.vectorize(H_2)\n",
    "    H_L = np.vectorize(H_L)\n",
    "    return H_2, H_L\n",
    "\n",
    "# ABMP parametrisation\n",
    "x_abmp = [0.0, 0.1, 0.3, 0.5, 0.7, 0.9, 1]\n",
    "\n",
    "\n",
    "included_proc = [\"DIS NC\"]\n",
    "excluded_exp = {\"DIS NC\" : [\"NMC_NC_NOTFIXED_DW_EM-F2\"]}\n",
    "included_exp = {}\n",
    "for proc in included_proc:\n",
    "    aux = []\n",
    "    for exp in process_info.namelist[proc]:\n",
    "        if exp not in excluded_exp[proc]:\n",
    "            aux.append(exp)\n",
    "    included_exp[proc] = aux\n",
    "\n",
    "preds_ht.loc[['DIS NC', 'DIS CC'], 'x'] = xvals_DIS\n",
    "preds_ht.loc[['DIS NC', 'DIS CC'], 'q2'] = q2vals_DIS\n",
    "preds_ht.loc[['DIS NC', 'DIS CC'], 'y'] = yvals_DIS\n",
    "\n",
    "# Initialise dataframe\n",
    "for i in range(len(x_abmp)):\n",
    "    preds_ht[f\"({i+1}+,0)\"] = 0\n",
    "    preds_ht[f\"(0,{i+1}+)\"] = 0\n",
    "\n",
    "deltas = defaultdict(list)\n",
    "\n",
    "for proc in process_info.namelist.keys():\n",
    "        for exp in process_info.namelist[proc]:\n",
    "            if proc in included_proc and exp in included_exp[proc]:\n",
    "                x  = np.array(preds_ht.xs(exp, level=1, drop_level=False).loc[:,\"x\"])\n",
    "                Q2 = np.array(preds_ht.xs(exp, level=1, drop_level=False).loc[:,\"q2\"])\n",
    "                y  = np.array(preds_ht.xs(exp, level=1, drop_level=False).loc[:,\"y\"])\n",
    "                N = np.array([])\n",
    "\n",
    "                if \"SIGMA\" in exp:\n",
    "                    N_2, N_L = compute_normalisation_by_experiment(exp, x, y, Q2)\n",
    "                elif \"F2\" in exp:\n",
    "                    N_2 = np.ones(shape=x.shape)\n",
    "                    N_L = np.zeros(shape=x.shape)\n",
    "                else:\n",
    "                    raise ValueError(f\"The normalisation for the observable is not known.\")\n",
    "\n",
    "                for i in range(len(x_abmp)):\n",
    "                    H_L, H_2 = wrapper_to_splines(i)\n",
    "                    deltas[f\"({i+1}+,0)\"] += [N_2 * H_2(x) / Q2]\n",
    "                    deltas[f\"(0,{i+1}+)\"] += [N_L * H_L(x) / Q2]\n",
    "            else:\n",
    "                for i in range(len(x_abmp)):\n",
    "                    deltas[f\"({i+1}+,0)\"] += [np.zeros(preds_ht.xs(exp, level=1, drop_level=False).shape[0])]\n",
    "                    deltas[f\"(0,{i+1}+)\"] += [np.zeros(preds_ht.xs(exp, level=1, drop_level=False).shape[0])]\n",
    "\n",
    "delta_pred = []\n",
    "for i in range(len(x_abmp)):\n",
    "    temp_1 = np.array([])\n",
    "    temp_2 = np.array([])\n",
    "    for vec in zip(deltas[f\"({i+1}+,0)\"], deltas[f\"(0,{i+1}+)\"]):\n",
    "        temp_1 = np.concatenate((temp_1, vec[0]))\n",
    "        temp_2 = np.concatenate((temp_2, vec[1]))\n",
    "    \n",
    "    preds_ht[f\"({i+1}+,0)\"] = temp_1\n",
    "    preds_ht[f\"(0,{i+1}+)\"] = temp_2\n",
    "    delta_pred.append(preds_ht[f\"({i+1}+,0)\"])\n",
    "    delta_pred.append(preds_ht[f\"(0,{i+1}+)\"])\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "800cd4de",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Theory covariance matrix\n",
    "S = np.zeros((delta_pred[0].size, delta_pred[0].size))\n",
    "for delta in delta_pred:\n",
    "    S += np.outer(delta, delta)\n",
    "\n",
    "S = pd.DataFrame(S, index=delta_pred[0].index, columns=delta_pred[0].index)\n",
    "\n",
    "# Experimental covariance matrix\n",
    "C = API.groups_covmat_no_table(**common_dict)\n",
    "\n",
    "# Ensure that S anc C are ordered in the same way (in practice they already are)\n",
    "S = S.reindex(C.index).T.reindex(C.index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "d896cf55",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the central value of the pseudodata\n",
    "# this is needed to compute the distance between prediction and data\n",
    "pseudodata = API.read_pdf_pseudodata(**common_dict)\n",
    "dat_central = np.mean(\n",
    "    [i.pseudodata.reindex(preds.index.to_list()).to_numpy().flatten() for i in pseudodata],\n",
    "    axis=0,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "d373edbc",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/l5/l1kl3n_11gx3zm7b0wxctqym0000gn/T/ipykernel_4744/871155213.py:34: RuntimeWarning: invalid value encountered in sqrt\n",
      "  uncs = np.sqrt(P_tilde)\n"
     ]
    }
   ],
   "source": [
    "# Compute delta_T_tilde (Eq. 3.37) and P_tilde (Eq. 3.38) of arXiv:2105.05114\n",
    "\n",
    "# The factors 1/sqrt(2) are to normalize for the fact that beta provides information about\n",
    "# theoretical uncertainties along two directions\n",
    "# CHECK THIS PART\n",
    "# b_tilde SHOULD BE INDEPENDENT OF THE PRIOR THAT WE USE TO MODEL HT CORRECTIONS.\n",
    "\n",
    "central_ht_coeffs = np.zeros(len(H2_coeff_list) + len(HL_coeff_list)) \n",
    "\n",
    "# Construct beta tilde\n",
    "H_single_list = np.concatenate((H2_coeff_list, HL_coeff_list))\n",
    "beta_tilde = []\n",
    "for i, par in enumerate(H_single_list):\n",
    "  aux = np.zeros(H_single_list.size)\n",
    "  aux[i] = par\n",
    "  beta_tilde.append(aux)\n",
    "\n",
    "S_tilde = np.zeros((len(beta_tilde[0]), len(beta_tilde[0])))\n",
    "for tilde in beta_tilde:\n",
    "    S_tilde += np.outer(tilde,tilde)\n",
    "\n",
    "beta = delta_pred\n",
    "S_hat = np.zeros((len(beta_tilde[0]),delta_pred[0].size))\n",
    "for b in zip(beta_tilde, beta):\n",
    "    S_hat += np.outer(b[0], b[1])\n",
    "\n",
    "invcov = np.linalg.inv(C + S)\n",
    "\n",
    "delta_T_tilde = -S_hat @ invcov @ (mean_prediction - dat_central)\n",
    "# where are the X_tilde and X_hat terms in P_tilde?\n",
    "# Maybe not present because we don't have correlations between theory parameters\n",
    "P_tilde = S_hat @ invcov @ X @ invcov @ S_hat.T + (S_tilde - S_hat @ invcov @ S_hat.T)\n",
    "preds = central_ht_coeffs + delta_T_tilde\n",
    "uncs = np.sqrt(P_tilde)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "f50ade72",
   "metadata": {},
   "outputs": [],
   "source": [
    "# check if the stored covmat is equal to S we recomputed above\n",
    "fitpath = API.fit(fit=fitname).path\n",
    "try:\n",
    "    stored_covmat = pd.read_csv(\n",
    "        fitpath / \"tables/datacuts_theory_theorycovmatconfig_user_covmat.csv\",\n",
    "        sep=\"\\t\",\n",
    "        encoding=\"utf-8\",\n",
    "        index_col=2,\n",
    "        header=3,\n",
    "        skip_blank_lines=False,\n",
    "    )\n",
    "except FileNotFoundError:\n",
    "    stored_covmat = pd.read_csv(\n",
    "        fitpath / \"tables/datacuts_theory_theorycovmatconfig_theory_covmat_custom.csv\",\n",
    "        index_col=[0, 1, 2],\n",
    "        header=[0, 1, 2],\n",
    "        sep=\"\\t|,\",\n",
    "        engine=\"python\",\n",
    "    ).fillna(0)\n",
    "    storedcovmat_index = pd.MultiIndex.from_tuples(\n",
    "        [(aa, bb, np.int64(cc)) for aa, bb, cc in stored_covmat.index],\n",
    "        names=[\"group\", \"dataset\", \"id\"],\n",
    "    )\n",
    "    stored_covmat = pd.DataFrame(\n",
    "        stored_covmat.values, index=storedcovmat_index, columns=storedcovmat_index\n",
    "    )\n",
    "    stored_covmat = stored_covmat.reindex(S.index).T.reindex(S.index)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "523f127f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reconstructed theory covmat, S, is not the same as the stored covmat!\n"
     ]
    }
   ],
   "source": [
    "# print the final result\n",
    "if np.allclose(S, stored_covmat):\n",
    "    for i, pred in enumerate(preds):\n",
    "        tpye = \"2\" if i < len(H2_coeff_list) else \"L\"\n",
    "        n = i%7\n",
    "        print(\n",
    "            f\"H_{tpye} node {n+1} = {preds[i]:.5f} ± {np.sqrt(P_tilde[i,i]):.5f} \\n\"\n",
    "        )\n",
    "        if i == len(H2_coeff_list)-1:\n",
    "            print(\"-----------------------------\\n\")\n",
    "else:\n",
    "    print(\"Reconstructed theory covmat, S, is not the same as the stored covmat!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "1ea3db29",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.046,\n",
       " 0.17485525141984382,\n",
       " 0.08792405790465052,\n",
       " -0.08716295870732806,\n",
       " 0.0430605400225677,\n",
       " 3.152504441681324e-05,\n",
       " 0.0]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "HL_coeff_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3fb4f74",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nnpdf",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  },
  "vscode": {
   "interpreter": {
    "hash": "589e3134e9d89160e5ace28972e8dc0b682f48816407b59cbfdad217f6fb745b"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

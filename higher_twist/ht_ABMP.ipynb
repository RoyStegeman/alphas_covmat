{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "226dc610",
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "from validphys.api import API\n",
    "import scipy as sp\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from scipy import interpolate as scint\n",
    "from collections import defaultdict, namedtuple\n",
    "import operator\n",
    "\n",
    "from validphys.theorycovariance.construction import extract_target, compute_ratio_delta, compute_ht_parametrisation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "92f93410",
   "metadata": {},
   "outputs": [],
   "source": [
    "fitname = \"240530-01-fABMP22R-CT\"\n",
    "thcovmat_dict = API.fit(fit=fitname).as_input()[\"theorycovmatconfig\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d7dae95b",
   "metadata": {},
   "outputs": [],
   "source": [
    "H2_coeff_list = thcovmat_dict[\"H2_list\"]\n",
    "HL_coeff_list = thcovmat_dict[\"HL_list\"]\n",
    "\n",
    "\n",
    "# dict used to produce theory predictions to construct the theory covmat as well as to produce\n",
    "# theory predictions from the fit performed using the ht covmat (i.e. the predicitons that should\n",
    "# be compared to data)\n",
    "common_dict = dict(\n",
    "    dataset_inputs={\"from_\": \"fit\"},\n",
    "    fit=fitname,\n",
    "    fits=[fitname],\n",
    "    use_cuts=\"fromfit\",\n",
    "    metadata_group=\"nnpdf31_process\",\n",
    "    theory={\"from_\": \"fit\"},\n",
    "    theoryid={\"from_\": \"theory\"},\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "08d0d9a3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LHAPDF 6.5.4 loading /opt/homebrew/Caskroom/miniconda/base/envs/nnpdf/share/LHAPDF/210619-n3fit-001/210619-n3fit-001_0000.dat\n",
      "210619-n3fit-001 PDF set, member #0, version 1\n",
      "LHAPDF 6.5.4 loading all 101 PDFs in set 210619-n3fit-001\n",
      "210619-n3fit-001, version 1; 101 PDF members\n"
     ]
    }
   ],
   "source": [
    "# collect the information (predictions + kinematics) needed for the computation of the HT covmat\n",
    "\n",
    "# Calculate theory predictions of the input PDF\n",
    "S_dict = dict(\n",
    "    theorycovmatconfig={\"from_\": \"fit\"},\n",
    "    pdf={\"from_\": \"theorycovmatconfig\"},\n",
    "    use_t0=True,\n",
    "    datacuts={\"from_\": \"fit\"},\n",
    "    t0pdfset={\"from_\": \"datacuts\"},\n",
    ")\n",
    "preds_ht_cov_construction = API.group_result_central_table_no_table(**(S_dict | common_dict))\n",
    "preds_ht = pd.DataFrame(preds_ht_cov_construction['theory_central'])\n",
    "\n",
    "# collect the corresponding kinemacs\n",
    "process_info = API.combine_by_type_ht(**(S_dict | common_dict))\n",
    "N_full_data = np.sum([i for i in process_info.sizes.values()])\n",
    "kinematics_DIS = np.concatenate([v for v in [process_info.data[\"DIS NC\"], process_info.data[\"DIS CC\"]]]).T\n",
    "# TO CHECK: IS preds[][1] THE THEORY PREDICTION?\n",
    "preds_DIS = np.concatenate([v for v in [process_info.preds[\"DIS NC\"][1], process_info.preds[\"DIS CC\"][1]]]).T\n",
    "xvals_DIS = kinematics_DIS[0]\n",
    "q2vals_DIS = kinematics_DIS[1]\n",
    "yvals_DIS = kinematics_DIS[2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "9c21605b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LHAPDF 6.5.4 loading all 501 PDFs in set 240530-01-fABMP22R-CT\n",
      "240530-01-fABMP22R-CT, version 1; 501 PDF members\n"
     ]
    }
   ],
   "source": [
    "# Calculate theory predictions of the fit with ht covmat - this will be compared to data\n",
    "preds = API.group_result_table_no_table(pdf={\"from_\": \"fit\"}, **common_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "bc820c3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# compute the matrix X encoding the PDF uncertainties of the predictions\n",
    "preds_onlyreplicas = preds.iloc[:, 2:].to_numpy()\n",
    "mean_prediction = np.mean(preds_onlyreplicas, axis=1)\n",
    "\n",
    "X = np.zeros((preds.shape[0], preds.shape[0]))\n",
    "for i in range(preds_onlyreplicas.shape[1]):\n",
    "    X += np.outer(\n",
    "        (preds_onlyreplicas[:, i] - mean_prediction), (preds_onlyreplicas[:, i] - mean_prediction)\n",
    "    )\n",
    "X *= 1 / preds_onlyreplicas.shape[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "76acc02c",
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.options.mode.chained_assignment = None\n",
    "\n",
    "data_by_process = API.groups_data_by_process(**(S_dict | common_dict))\n",
    "PDF_thcovmat = API.pdf(**(S_dict | common_dict))\n",
    "\n",
    "# ABMP parametrisationa\n",
    "x_abmp = [0.0, 0.1, 0.3, 0.5, 0.7, 0.9, 1]\n",
    "\n",
    "\n",
    "included_proc = [\"DIS NC\"]\n",
    "excluded_exp = {\"DIS NC\" : []}\n",
    "included_exp = {}\n",
    "for proc in included_proc:\n",
    "    aux = []\n",
    "    for exp in process_info.namelist[proc]:\n",
    "        if exp not in excluded_exp[proc]:\n",
    "            aux.append(exp)\n",
    "    included_exp[proc] = aux\n",
    "\n",
    "preds_ht.loc[['DIS NC', 'DIS CC'], 'x'] = xvals_DIS\n",
    "preds_ht.loc[['DIS NC', 'DIS CC'], 'q2'] = q2vals_DIS\n",
    "preds_ht.loc[['DIS NC', 'DIS CC'], 'y'] = yvals_DIS\n",
    "\n",
    "# Initialise dataframe\n",
    "for i in range(len(x_abmp)):\n",
    "    preds_ht[f\"p({i+1}+,0)\"] = 0\n",
    "    preds_ht[f\"p(0,{i+1}+)\"] = 0\n",
    "    preds_ht[f\"d({i+1}+,0)\"] = 0\n",
    "    preds_ht[f\"d(0,{i+1}+)\"] = 0\n",
    "\n",
    "deltas = defaultdict(list)\n",
    "\n",
    "for i_proc, proc in enumerate(process_info.namelist.keys()):\n",
    "        for i_exp, exp in enumerate(process_info.namelist[proc]):\n",
    "            dataset = data_by_process[i_proc].datasets[i_exp]\n",
    "            kin_dict = {}\n",
    "\n",
    "            if proc in included_proc and exp in included_exp[proc]:\n",
    "                kin_dict['x']  = np.array(preds_ht.xs(exp, level=1, drop_level=False).loc[:,\"x\"])\n",
    "                kin_dict['Q2'] = np.array(preds_ht.xs(exp, level=1, drop_level=False).loc[:,\"q2\"])\n",
    "                kin_dict['y']  = np.array(preds_ht.xs(exp, level=1, drop_level=False).loc[:,\"y\"])\n",
    "                kin_size =  kin_dict['x'].size\n",
    "                target = extract_target(dataset)\n",
    "\n",
    "\n",
    "                # Loop over the parameter\n",
    "                for i in range(len(x_abmp)):\n",
    "                    PC_2, PC_L = compute_ht_parametrisation(i, x_abmp, kin_dict, exp, H2_coeff_list, HL_coeff_list)\n",
    "                    if target == 'proton':\n",
    "                      deltas[f\"p({i+1}+,0)\"] += [PC_2]\n",
    "                      deltas[f\"p(0,{i+1}+)\"] += [PC_L]\n",
    "                      deltas[f\"d({i+1}+,0)\"] += [np.zeros(kin_size)]\n",
    "                      deltas[f\"d(0,{i+1}+)\"] += [np.zeros(kin_size)]\n",
    "                    elif target == 'deuteron':\n",
    "                      deltas[f\"p({i+1}+,0)\"] += [np.zeros(kin_size)]\n",
    "                      deltas[f\"p(0,{i+1}+)\"] += [np.zeros(kin_size)]\n",
    "                      deltas[f\"d({i+1}+,0)\"] += [PC_2]\n",
    "                      deltas[f\"d(0,{i+1}+)\"] += [PC_L]\n",
    "                    elif target == 'ratio':\n",
    "                      deltas[f\"p({i+1}+,0)\"] += [compute_ratio_delta(dataset, PDF_thcovmat, \"p\", PC_2) - compute_ratio_delta(dataset, PDF_thcovmat)]\n",
    "                      deltas[f\"p(0,{i+1}+)\"] += [compute_ratio_delta(dataset, PDF_thcovmat, \"p\", PC_L) - compute_ratio_delta(dataset, PDF_thcovmat)]\n",
    "                      deltas[f\"d({i+1}+,0)\"] += [compute_ratio_delta(dataset, PDF_thcovmat, \"d\", PC_2) - compute_ratio_delta(dataset, PDF_thcovmat)]\n",
    "                      deltas[f\"d(0,{i+1}+)\"] += [compute_ratio_delta(dataset, PDF_thcovmat, \"d\", PC_L) - compute_ratio_delta(dataset, PDF_thcovmat)]\n",
    "                    else:\n",
    "                        raise ValueError(\"Could not detect target.\")\n",
    "            else:\n",
    "                for i in range(len(x_abmp)):\n",
    "                    deltas[f\"p({i+1}+,0)\"] += [np.zeros(preds_ht.xs(exp, level=1, drop_level=False).shape[0])]\n",
    "                    deltas[f\"p(0,{i+1}+)\"] += [np.zeros(preds_ht.xs(exp, level=1, drop_level=False).shape[0])]\n",
    "                    deltas[f\"d({i+1}+,0)\"] += [np.zeros(preds_ht.xs(exp, level=1, drop_level=False).shape[0])]\n",
    "                    deltas[f\"d(0,{i+1}+)\"] += [np.zeros(preds_ht.xs(exp, level=1, drop_level=False).shape[0])]\n",
    "\n",
    "delta_pred = []\n",
    "for i in range(len(x_abmp)):\n",
    "    temp_1 = np.array([])\n",
    "    temp_2 = np.array([])\n",
    "    temp_3 = np.array([])\n",
    "    temp_4 = np.array([])\n",
    "    for vec in zip(deltas[f\"p({i+1}+,0)\"], deltas[f\"p(0,{i+1}+)\"], deltas[f\"d({i+1}+,0)\"], deltas[f\"d(0,{i+1}+)\"]):\n",
    "        temp_1 = np.concatenate((temp_1, vec[0]))\n",
    "        temp_2 = np.concatenate((temp_2, vec[1]))\n",
    "        temp_3 = np.concatenate((temp_3, vec[2]))\n",
    "        temp_4 = np.concatenate((temp_4, vec[3]))\n",
    "    \n",
    "    preds_ht[f\"p({i+1}+,0)\"] = temp_1\n",
    "    preds_ht[f\"p(0,{i+1}+)\"] = temp_2\n",
    "    preds_ht[f\"d({i+1}+,0)\"] = temp_3\n",
    "    preds_ht[f\"d(0,{i+1}+)\"] = temp_4\n",
    "    delta_pred.append(preds_ht[f\"p({i+1}+,0)\"])\n",
    "    delta_pred.append(preds_ht[f\"p(0,{i+1}+)\"])\n",
    "    delta_pred.append(preds_ht[f\"d({i+1}+,0)\"])\n",
    "    delta_pred.append(preds_ht[f\"d(0,{i+1}+)\"])\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "800cd4de",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Theory covariance matrix\n",
    "S = np.zeros((delta_pred[0].size, delta_pred[0].size))\n",
    "for delta in delta_pred:\n",
    "    S += np.outer(delta, delta)\n",
    "\n",
    "S = pd.DataFrame(S, index=delta_pred[0].index, columns=delta_pred[0].index)\n",
    "\n",
    "# Experimental covariance matrix\n",
    "C = API.groups_covmat_no_table(**common_dict)\n",
    "\n",
    "# Ensure that S anc C are ordered in the same way (in practice they already are)\n",
    "S = S.reindex(C.index).T.reindex(C.index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d896cf55",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the central value of the pseudodata\n",
    "# this is needed to compute the distance between prediction and data\n",
    "pseudodata = API.read_pdf_pseudodata(**common_dict)\n",
    "dat_central = np.mean(\n",
    "    [i.pseudodata.reindex(preds.index.to_list()).to_numpy().flatten() for i in pseudodata],\n",
    "    axis=0,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d373edbc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute delta_T_tilde (Eq. 3.37) and P_tilde (Eq. 3.38) of arXiv:2105.05114\n",
    "\n",
    "# The factors 1/sqrt(2) are to normalize for the fact that beta provides information about\n",
    "# theoretical uncertainties along two directions\n",
    "# CHECK THIS PART\n",
    "# b_tilde SHOULD BE INDEPENDENT OF THE PRIOR THAT WE USE TO MODEL HT CORRECTIONS.\n",
    "\n",
    "central_ht_coeffs = np.zeros(len(H2_coeff_list) + len(HL_coeff_list)) \n",
    "\n",
    "# Construct beta tilde\n",
    "H_single_list = np.concatenate((H2_coeff_list, HL_coeff_list))\n",
    "beta_tilde = []\n",
    "for i, par in enumerate(H_single_list):\n",
    "  aux = np.zeros(H_single_list.size)\n",
    "  aux[i] = par\n",
    "  beta_tilde.append(aux)\n",
    "\n",
    "S_tilde = np.zeros((len(beta_tilde[0]), len(beta_tilde[0])))\n",
    "for tilde in beta_tilde:\n",
    "    S_tilde += np.outer(tilde,tilde)\n",
    "\n",
    "beta = delta_pred\n",
    "S_hat = np.zeros((len(beta_tilde[0]),delta_pred[0].size))\n",
    "for b in zip(beta_tilde, beta):\n",
    "    S_hat += np.outer(b[0], b[1])\n",
    "\n",
    "invcov = np.linalg.inv(C + S)\n",
    "\n",
    "delta_T_tilde = -S_hat @ invcov @ (mean_prediction - dat_central)\n",
    "# where are the X_tilde and X_hat terms in P_tilde?\n",
    "# Maybe not present because we don't have correlations between theory parameters\n",
    "P_tilde = S_hat @ invcov @ X @ invcov @ S_hat.T + (S_tilde - S_hat @ invcov @ S_hat.T)\n",
    "preds = central_ht_coeffs + delta_T_tilde\n",
    "uncs = np.sqrt(P_tilde)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f50ade72",
   "metadata": {},
   "outputs": [],
   "source": [
    "# check if the stored covmat is equal to S we recomputed above\n",
    "fitpath = API.fit(fit=fitname).path\n",
    "try:\n",
    "    stored_covmat = pd.read_csv(\n",
    "        fitpath / \"tables/datacuts_theory_theorycovmatconfig_user_covmat.csv\",\n",
    "        sep=\"\\t\",\n",
    "        encoding=\"utf-8\",\n",
    "        index_col=2,\n",
    "        header=3,\n",
    "        skip_blank_lines=False,\n",
    "    )\n",
    "except FileNotFoundError:\n",
    "    stored_covmat = pd.read_csv(\n",
    "        fitpath / \"tables/datacuts_theory_theorycovmatconfig_theory_covmat_custom.csv\",\n",
    "        index_col=[0, 1, 2],\n",
    "        header=[0, 1, 2],\n",
    "        sep=\"\\t|,\",\n",
    "        engine=\"python\",\n",
    "    ).fillna(0)\n",
    "    storedcovmat_index = pd.MultiIndex.from_tuples(\n",
    "        [(aa, bb, np.int64(cc)) for aa, bb, cc in stored_covmat.index],\n",
    "        names=[\"group\", \"dataset\", \"id\"],\n",
    "    )\n",
    "    stored_covmat = pd.DataFrame(\n",
    "        stored_covmat.values, index=storedcovmat_index, columns=storedcovmat_index\n",
    "    )\n",
    "    stored_covmat = stored_covmat.reindex(S.index).T.reindex(S.index)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "523f127f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# print the final result\n",
    "if np.allclose(S, stored_covmat):\n",
    "    print(\n",
    "        f\"Reversed 5pt\\n\"\n",
    "        f\"-----------------------------\\n\"\n",
    "        )\n",
    "    for i, pred in enumerate(preds):\n",
    "        tpye = \"2\" if i < len(H2_coeff_list) else \"L\"\n",
    "        n = i%7\n",
    "        print(\n",
    "            f\"H_{tpye} node {n+1} = {preds[i]:.5f} ± {np.sqrt(P_tilde[i,i]):.5f} \\n\"\n",
    "        )\n",
    "        if i == len(H2_coeff_list)-1:\n",
    "            print(\"-----------------------------\\n\")\n",
    "else:\n",
    "    print(\"Reconstructed theory covmat, S, is not the same as the stored covmat!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0c2d826",
   "metadata": {},
   "source": [
    "# Plots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31453642",
   "metadata": {},
   "outputs": [],
   "source": [
    "y2_central = [preds[i] for i in range(len(H2_coeff_list))]\n",
    "y2_sigma = [np.sqrt(P_tilde[i,i]) for i in range(len(H2_coeff_list))]\n",
    "y2_plus = [x1 + x2 for x1,x2 in zip(y2_central, y2_sigma)]\n",
    "y2_minus = [x1 - x2 for x1,x2 in zip(y2_central, y2_sigma)]\n",
    "\n",
    "yL_central = [preds[i] for i in range(len(H2_coeff_list), len(HL_coeff_list) + len(H2_coeff_list))]\n",
    "yL_sigma = [np.sqrt(P_tilde[i,i]) for i in range(len(H2_coeff_list), len(HL_coeff_list) + len(H2_coeff_list))]\n",
    "yL_plus = [x1 + x2 for x1,x2 in zip(yL_central, yL_sigma)]\n",
    "yL_minus = [x1 - x2 for x1,x2 in zip(yL_central, yL_sigma)]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16f21817",
   "metadata": {},
   "outputs": [],
   "source": [
    "H2 = sp.interpolate.CubicSpline(x_abmp, y2_central)\n",
    "H2_plus = sp.interpolate.CubicSpline(x_abmp, y2_plus)\n",
    "H2_minus = sp.interpolate.CubicSpline(x_abmp, y2_minus)\n",
    "H2_color = \"sandybrown\"\n",
    "H2_label = \"H2\"\n",
    "\n",
    "\n",
    "HL = sp.interpolate.CubicSpline(x_abmp, yL_central)\n",
    "HL_plus = sp.interpolate.CubicSpline(x_abmp, yL_plus)\n",
    "HL_minus = sp.interpolate.CubicSpline(x_abmp, yL_minus)\n",
    "HL_color = \"green\"\n",
    "HL_label = \"HL\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "00a1aa80",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_wrapper(H, H_p, H_m, y, label, color, ylabel):\n",
    "  xv = np.logspace(-5, -0.0001, 100)\n",
    "  legends = []\n",
    "  legend_name = [label, \"knots\"]\n",
    "  fig, ax = plt.subplots(figsize=(12.5, 8))\n",
    "  knots = ax.plot(x_abmp, y, 'o', label='data')\n",
    "  pl = ax.plot(xv, H(xv), ls = \"-\", lw = 1, color=color)\n",
    "  pl_lg= ax.fill(np.NaN, np.NaN, color = color, alpha = 0.3) # Necessary for fancy legend\n",
    "  legends.append((pl[0], pl_lg[0]))\n",
    "  legends.append(knots[0])\n",
    "  ax.fill_between(xv, H_p(xv), H_m(xv), color = color, alpha = 0.3)\n",
    "  ax.set_xscale(\"log\")\n",
    "  ax.set_xlabel(f'$x$')\n",
    "  ax.set_ylabel(ylabel)\n",
    "  \n",
    "  \n",
    "  fig.legend(legends, legend_name, loc=[0.1,0.15], fontsize=15)\n",
    "  plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd78dc44",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_wrapper(H2, H2_plus, H2_minus, y2_central, label=r\"$H_2 \\pm \\sigma$\", color=H2_color, ylabel=r\"$H_2$\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "393c4847",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_wrapper(HL, HL_plus, HL_minus, yL_central, label=r\"$H_L \\pm \\sigma$\", color=HL_color, ylabel=r\"$H_L$\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axs = plt.subplots(1,2, figsize=(15, 7))\n",
    "H2_dict = {\n",
    "  \"func\" : H2,\n",
    "  \"func_plus_std\": H2_plus,\n",
    "  \"func_minus_std\": H2_minus,\n",
    "  \"knots\": y2_central,\n",
    "  \"label\": r\"$H_2 \\pm \\sigma$\",\n",
    "  \"ylabel\": r\"$H_2$\",\n",
    "  \"color\": H2_color\n",
    "}\n",
    "\n",
    "HL_dict = {\n",
    "  \"func\" : HL,\n",
    "  \"func_plus_std\": HL_plus,\n",
    "  \"func_minus_std\": HL_minus,\n",
    "  \"knots\": yL_central,\n",
    "  \"label\": r\"$H_L \\pm \\sigma$\",\n",
    "  \"ylabel\": r\"$H_L$\",\n",
    "  \"color\": HL_color\n",
    "}\n",
    "dicts = [H2_dict, HL_dict]\n",
    "\n",
    "def plot_wrapper(H, H_p, H_m, y, label, color, ylabel, ax):\n",
    "  xv = np.logspace(-5, -0.0001, 100)\n",
    "  legends = []\n",
    "  legend_name = [label, \"knots\"]\n",
    "  knots = ax.plot(x_abmp, y, 'o', label='data')\n",
    "  pl = ax.plot(xv, H(xv), ls = \"-\", lw = 1, color=color)\n",
    "  pl_lg= ax.fill(np.NaN, np.NaN, color = color, alpha = 0.3) # Necessary for fancy legend\n",
    "  legends.append((pl[0], pl_lg[0]))\n",
    "  legends.append(knots[0])\n",
    "  ax.fill_between(xv, H_p(xv), H_m(xv), color = color, alpha = 0.3)\n",
    "  ax.set_xscale(\"log\")\n",
    "  ax.set_xlabel(f'$x$')\n",
    "  ax.set_ylabel(ylabel)\n",
    "  ax.legend(legends, legend_name, loc=[0.1,0.15], fontsize=15)\n",
    "\n",
    "for i, HTdict in enumerate(dicts):\n",
    "  plot_wrapper(H=HTdict[\"func\"],\n",
    "               H_p=HTdict[\"func_plus_std\"],\n",
    "               H_m=HTdict[\"func_minus_std\"],\n",
    "               y=HTdict[\"knots\"],\n",
    "               label=HTdict[\"label\"],\n",
    "               color=HTdict[\"color\"],\n",
    "               ylabel=HTdict[\"ylabel\"],\n",
    "               ax=axs[i])\n",
    "\n",
    "axs[0].text(5.e-5, 0.06, fitname, fontsize=20)\n",
    "fig.savefig()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d960e863",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nnpdf",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  },
  "vscode": {
   "interpreter": {
    "hash": "589e3134e9d89160e5ace28972e8dc0b682f48816407b59cbfdad217f6fb745b"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

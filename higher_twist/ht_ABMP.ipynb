{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "226dc610",
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "from validphys.api import API\n",
    "import scipy as sp\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from scipy import interpolate as scint\n",
    "from collections import defaultdict, namedtuple\n",
    "import operator\n",
    "\n",
    "from validphys.theorycovariance.construction import extract_target, compute_ratio_delta, compute_ht_parametrisation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "92f93410",
   "metadata": {},
   "outputs": [],
   "source": [
    "fitname = \"240530-01-fABMP22R-CT\"\n",
    "thcovmat_dict = API.fit(fit=fitname).as_input()[\"theorycovmatconfig\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d7dae95b",
   "metadata": {},
   "outputs": [],
   "source": [
    "H2_coeff_list = thcovmat_dict[\"H2_list\"]\n",
    "HL_coeff_list = thcovmat_dict[\"HL_list\"]\n",
    "\n",
    "\n",
    "# dict used to produce theory predictions to construct the theory covmat as well as to produce\n",
    "# theory predictions from the fit performed using the ht covmat (i.e. the predicitons that should\n",
    "# be compared to data)\n",
    "common_dict = dict(\n",
    "    dataset_inputs={\"from_\": \"fit\"},\n",
    "    fit=fitname,\n",
    "    fits=[fitname],\n",
    "    use_cuts=\"fromfit\",\n",
    "    metadata_group=\"nnpdf31_process\",\n",
    "    theory={\"from_\": \"fit\"},\n",
    "    theoryid={\"from_\": \"theory\"},\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "08d0d9a3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LHAPDF 6.5.4 loading /opt/homebrew/Caskroom/miniconda/base/envs/nnpdf/share/LHAPDF/210619-n3fit-001/210619-n3fit-001_0000.dat\n",
      "210619-n3fit-001 PDF set, member #0, version 1\n",
      "LHAPDF 6.5.4 loading all 101 PDFs in set 210619-n3fit-001\n",
      "210619-n3fit-001, version 1; 101 PDF members\n"
     ]
    }
   ],
   "source": [
    "# collect the information (predictions + kinematics) needed for the computation of the HT covmat\n",
    "\n",
    "# Calculate theory predictions of the input PDF\n",
    "S_dict = dict(\n",
    "    theorycovmatconfig={\"from_\": \"fit\"},\n",
    "    pdf={\"from_\": \"theorycovmatconfig\"},\n",
    "    use_t0=True,\n",
    "    datacuts={\"from_\": \"fit\"},\n",
    "    t0pdfset={\"from_\": \"datacuts\"},\n",
    ")\n",
    "preds_ht_cov_construction = API.group_result_central_table_no_table(**(S_dict | common_dict))\n",
    "preds_ht = pd.DataFrame(preds_ht_cov_construction['theory_central'])\n",
    "\n",
    "# collect the corresponding kinemacs\n",
    "process_info = API.combine_by_type_ht(**(S_dict | common_dict))\n",
    "N_full_data = np.sum([i for i in process_info.sizes.values()])\n",
    "kinematics_DIS = np.concatenate([v for v in [process_info.data[\"DIS NC\"], process_info.data[\"DIS CC\"]]]).T\n",
    "# TO CHECK: IS preds[][1] THE THEORY PREDICTION?\n",
    "preds_DIS = np.concatenate([v for v in [process_info.preds[\"DIS NC\"][1], process_info.preds[\"DIS CC\"][1]]]).T\n",
    "xvals_DIS = kinematics_DIS[0]\n",
    "q2vals_DIS = kinematics_DIS[1]\n",
    "yvals_DIS = kinematics_DIS[2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "9c21605b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LHAPDF 6.5.4 loading all 501 PDFs in set 240530-01-fABMP22R-CT\n",
      "240530-01-fABMP22R-CT, version 1; 501 PDF members\n"
     ]
    }
   ],
   "source": [
    "# Calculate theory predictions of the fit with ht covmat - this will be compared to data\n",
    "preds = API.group_result_table_no_table(pdf={\"from_\": \"fit\"}, **common_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "bc820c3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# compute the matrix X encoding the PDF uncertainties of the predictions\n",
    "preds_onlyreplicas = preds.iloc[:, 2:].to_numpy()\n",
    "mean_prediction = np.mean(preds_onlyreplicas, axis=1)\n",
    "\n",
    "X = np.zeros((preds.shape[0], preds.shape[0]))\n",
    "for i in range(preds_onlyreplicas.shape[1]):\n",
    "    X += np.outer(\n",
    "        (preds_onlyreplicas[:, i] - mean_prediction), (preds_onlyreplicas[:, i] - mean_prediction)\n",
    "    )\n",
    "X *= 1 / preds_onlyreplicas.shape[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "76acc02c",
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Length of values (2516) does not match length of index (4616)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[8], line 79\u001b[0m\n\u001b[1;32m     76\u001b[0m     temp_1 \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mconcatenate((temp_1, vec[\u001b[38;5;241m0\u001b[39m]))\n\u001b[1;32m     77\u001b[0m     temp_2 \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mconcatenate((temp_2, vec[\u001b[38;5;241m1\u001b[39m]))\n\u001b[0;32m---> 79\u001b[0m \u001b[43mpreds_ht\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43mf\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m(\u001b[39;49m\u001b[38;5;132;43;01m{\u001b[39;49;00m\u001b[43mi\u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[38;5;132;43;01m}\u001b[39;49;00m\u001b[38;5;124;43m+,0)\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m \u001b[38;5;241m=\u001b[39m temp_1\n\u001b[1;32m     80\u001b[0m preds_ht[\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m(0,\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mi\u001b[38;5;241m+\u001b[39m\u001b[38;5;241m1\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m+)\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m temp_2\n\u001b[1;32m     81\u001b[0m delta_pred\u001b[38;5;241m.\u001b[39mappend(preds_ht[\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m(\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mi\u001b[38;5;241m+\u001b[39m\u001b[38;5;241m1\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m+,0)\u001b[39m\u001b[38;5;124m\"\u001b[39m])\n",
      "File \u001b[0;32m/opt/homebrew/Caskroom/miniconda/base/envs/nnpdf/lib/python3.12/site-packages/pandas/core/frame.py:4311\u001b[0m, in \u001b[0;36mDataFrame.__setitem__\u001b[0;34m(self, key, value)\u001b[0m\n\u001b[1;32m   4308\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_setitem_array([key], value)\n\u001b[1;32m   4309\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   4310\u001b[0m     \u001b[38;5;66;03m# set column\u001b[39;00m\n\u001b[0;32m-> 4311\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_set_item\u001b[49m\u001b[43m(\u001b[49m\u001b[43mkey\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvalue\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/homebrew/Caskroom/miniconda/base/envs/nnpdf/lib/python3.12/site-packages/pandas/core/frame.py:4524\u001b[0m, in \u001b[0;36mDataFrame._set_item\u001b[0;34m(self, key, value)\u001b[0m\n\u001b[1;32m   4514\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_set_item\u001b[39m(\u001b[38;5;28mself\u001b[39m, key, value) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m   4515\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m   4516\u001b[0m \u001b[38;5;124;03m    Add series to DataFrame in specified column.\u001b[39;00m\n\u001b[1;32m   4517\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   4522\u001b[0m \u001b[38;5;124;03m    ensure homogeneity.\u001b[39;00m\n\u001b[1;32m   4523\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m-> 4524\u001b[0m     value, refs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_sanitize_column\u001b[49m\u001b[43m(\u001b[49m\u001b[43mvalue\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   4526\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[1;32m   4527\u001b[0m         key \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcolumns\n\u001b[1;32m   4528\u001b[0m         \u001b[38;5;129;01mand\u001b[39;00m value\u001b[38;5;241m.\u001b[39mndim \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m   4529\u001b[0m         \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(value\u001b[38;5;241m.\u001b[39mdtype, ExtensionDtype)\n\u001b[1;32m   4530\u001b[0m     ):\n\u001b[1;32m   4531\u001b[0m         \u001b[38;5;66;03m# broadcast across multiple columns if necessary\u001b[39;00m\n\u001b[1;32m   4532\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcolumns\u001b[38;5;241m.\u001b[39mis_unique \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcolumns, MultiIndex):\n",
      "File \u001b[0;32m/opt/homebrew/Caskroom/miniconda/base/envs/nnpdf/lib/python3.12/site-packages/pandas/core/frame.py:5266\u001b[0m, in \u001b[0;36mDataFrame._sanitize_column\u001b[0;34m(self, value)\u001b[0m\n\u001b[1;32m   5263\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m _reindex_for_setitem(value, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mindex)\n\u001b[1;32m   5265\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m is_list_like(value):\n\u001b[0;32m-> 5266\u001b[0m     \u001b[43mcom\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrequire_length_match\u001b[49m\u001b[43m(\u001b[49m\u001b[43mvalue\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mindex\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   5267\u001b[0m arr \u001b[38;5;241m=\u001b[39m sanitize_array(value, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mindex, copy\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m, allow_2d\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[1;32m   5268\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[1;32m   5269\u001b[0m     \u001b[38;5;28misinstance\u001b[39m(value, Index)\n\u001b[1;32m   5270\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m value\u001b[38;5;241m.\u001b[39mdtype \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mobject\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   5273\u001b[0m     \u001b[38;5;66;03m# TODO: Remove kludge in sanitize_array for string mode when enforcing\u001b[39;00m\n\u001b[1;32m   5274\u001b[0m     \u001b[38;5;66;03m# this deprecation\u001b[39;00m\n",
      "File \u001b[0;32m/opt/homebrew/Caskroom/miniconda/base/envs/nnpdf/lib/python3.12/site-packages/pandas/core/common.py:573\u001b[0m, in \u001b[0;36mrequire_length_match\u001b[0;34m(data, index)\u001b[0m\n\u001b[1;32m    569\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    570\u001b[0m \u001b[38;5;124;03mCheck the length of data matches the length of the index.\u001b[39;00m\n\u001b[1;32m    571\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    572\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(data) \u001b[38;5;241m!=\u001b[39m \u001b[38;5;28mlen\u001b[39m(index):\n\u001b[0;32m--> 573\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m    574\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mLength of values \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    575\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m(\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mlen\u001b[39m(data)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m) \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    576\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdoes not match length of index \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    577\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m(\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mlen\u001b[39m(index)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m)\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    578\u001b[0m     )\n",
      "\u001b[0;31mValueError\u001b[0m: Length of values (2516) does not match length of index (4616)"
     ]
    }
   ],
   "source": [
    "pd.options.mode.chained_assignment = None\n",
    "\n",
    "data_by_process = API.groups_data_by_process(**(S_dict | common_dict))\n",
    "PDF_thcovmat = API.pdf(**(S_dict | common_dict))\n",
    "\n",
    "# ABMP parametrisationa\n",
    "x_abmp = [0.0, 0.1, 0.3, 0.5, 0.7, 0.9, 1]\n",
    "\n",
    "\n",
    "included_proc = [\"DIS NC\"]\n",
    "excluded_exp = {\"DIS NC\" : []}\n",
    "included_exp = {}\n",
    "for proc in included_proc:\n",
    "    aux = []\n",
    "    for exp in process_info.namelist[proc]:\n",
    "        if exp not in excluded_exp[proc]:\n",
    "            aux.append(exp)\n",
    "    included_exp[proc] = aux\n",
    "\n",
    "preds_ht.loc[['DIS NC', 'DIS CC'], 'x'] = xvals_DIS\n",
    "preds_ht.loc[['DIS NC', 'DIS CC'], 'q2'] = q2vals_DIS\n",
    "preds_ht.loc[['DIS NC', 'DIS CC'], 'y'] = yvals_DIS\n",
    "\n",
    "# Initialise dataframe\n",
    "for i in range(len(x_abmp)):\n",
    "    preds_ht[f\"p({i+1}+,0)\"] = 0\n",
    "    preds_ht[f\"p(0,{i+1}+)\"] = 0\n",
    "    preds_ht[f\"d({i+1}+,0)\"] = 0\n",
    "    preds_ht[f\"d(0,{i+1}+)\"] = 0\n",
    "\n",
    "deltas = defaultdict(list)\n",
    "\n",
    "for i_proc, proc in enumerate(process_info.namelist.keys()):\n",
    "        for i_exp, exp in enumerate(process_info.namelist[proc]):\n",
    "            dataset = data_by_process[i_proc].datasets[i_exp]\n",
    "            kin_dict = {}\n",
    "\n",
    "            if proc in included_proc and exp in included_exp[proc]:\n",
    "                kin_dict['x']  = np.array(preds_ht.xs(exp, level=1, drop_level=False).loc[:,\"x\"])\n",
    "                kin_dict['Q2'] = np.array(preds_ht.xs(exp, level=1, drop_level=False).loc[:,\"q2\"])\n",
    "                kin_dict['y']  = np.array(preds_ht.xs(exp, level=1, drop_level=False).loc[:,\"y\"])\n",
    "                kin_size =  kin_dict['x'].size\n",
    "                target = extract_target(dataset)\n",
    "\n",
    "\n",
    "                # Loop over the parameter\n",
    "                for i in range(len(x_abmp)):\n",
    "                    PC_2, PC_L = compute_ht_parametrisation(i, x_abmp, kin_dict, exp, H2_coeff_list, HL_coeff_list)\n",
    "                    if target == 'proton':\n",
    "                      deltas[f\"p({i+1}+,0)\"] += [PC_2]\n",
    "                      deltas[f\"p(0,{i+1}+)\"] += [PC_L]\n",
    "                      deltas[f\"d({i+1}+,0)\"] += [np.zeros(kin_size)]\n",
    "                      deltas[f\"d(0,{i+1}+)\"] += [np.zeros(kin_size)]\n",
    "                    elif target == 'deuteron':\n",
    "                      deltas[f\"p({i+1}+,0)\"] += [np.zeros(kin_size)]\n",
    "                      deltas[f\"p(0,{i+1}+)\"] += [np.zeros(kin_size)]\n",
    "                      deltas[f\"d({i+1}+,0)\"] += [PC_2]\n",
    "                      deltas[f\"d(0,{i+1}+)\"] += [PC_L]\n",
    "                    elif target == 'ratio':\n",
    "                      deltas[f\"p({i+1}+,0)\"] += [compute_ratio_delta(dataset, PDF_thcovmat, \"p\", PC_2)]\n",
    "                      deltas[f\"p(0,{i+1}+)\"] += [compute_ratio_delta(dataset, PDF_thcovmat, \"p\", PC_L)]\n",
    "                      deltas[f\"d({i+1}+,0)\"] += [compute_ratio_delta(dataset, PDF_thcovmat, \"d\", PC_2)]\n",
    "                      deltas[f\"d(0,{i+1}+)\"] += [compute_ratio_delta(dataset, PDF_thcovmat, \"d\", PC_L)]\n",
    "                    else:\n",
    "                        raise ValueError(\"Could not detect target.\")\n",
    "            else:\n",
    "                for i in range(len(x_abmp)):\n",
    "                    deltas[f\"p({i+1}+,0)\"] += [np.zeros(preds_ht.xs(exp, level=1, drop_level=False).shape[0])]\n",
    "                    deltas[f\"p(0,{i+1}+)\"] += [np.zeros(preds_ht.xs(exp, level=1, drop_level=False).shape[0])]\n",
    "                    deltas[f\"d({i+1}+,0)\"] += [np.zeros(preds_ht.xs(exp, level=1, drop_level=False).shape[0])]\n",
    "                    deltas[f\"d(0,{i+1}+)\"] += [np.zeros(preds_ht.xs(exp, level=1, drop_level=False).shape[0])]\n",
    "\n",
    "delta_pred = []\n",
    "for i in range(len(x_abmp)):\n",
    "    temp_1 = np.array([])\n",
    "    temp_2 = np.array([])\n",
    "    for vec in zip(deltas[f\"p({i+1}+,0)\"], deltas[f\"p(0,{i+1}+)\"], deltas[f\"d({i+1}+,0)\"], deltas[f\"d(0,{i+1}+)\"]):\n",
    "        temp_1 = np.concatenate((temp_1, vec[0]))\n",
    "        temp_2 = np.concatenate((temp_2, vec[1]))\n",
    "        temp_3 = np.concatenate((temp_3, vec[2]))\n",
    "        temp_4 = np.concatenate((temp_4, vec[3]))\n",
    "    \n",
    "    preds_ht[f\"p({i+1}+,0)\"] = temp_1\n",
    "    preds_ht[f\"p(0,{i+1}+)\"] = temp_2\n",
    "    preds_ht[f\"d({i+1}+,0)\"] = temp_3\n",
    "    preds_ht[f\"d(0,{i+1}+)\"] = temp_4\n",
    "    delta_pred.append(preds_ht[f\"p({i+1}+,0)\"])\n",
    "    delta_pred.append(preds_ht[f\"p(0,{i+1}+)\"])\n",
    "    delta_pred.append(preds_ht[f\"d({i+1}+,0)\"])\n",
    "    delta_pred.append(preds_ht[f\"d(0,{i+1}+)\"])\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "800cd4de",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Theory covariance matrix\n",
    "S = np.zeros((delta_pred[0].size, delta_pred[0].size))\n",
    "for delta in delta_pred:\n",
    "    S += np.outer(delta, delta)\n",
    "\n",
    "S = pd.DataFrame(S, index=delta_pred[0].index, columns=delta_pred[0].index)\n",
    "\n",
    "# Experimental covariance matrix\n",
    "C = API.groups_covmat_no_table(**common_dict)\n",
    "\n",
    "# Ensure that S anc C are ordered in the same way (in practice they already are)\n",
    "S = S.reindex(C.index).T.reindex(C.index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d896cf55",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the central value of the pseudodata\n",
    "# this is needed to compute the distance between prediction and data\n",
    "pseudodata = API.read_pdf_pseudodata(**common_dict)\n",
    "dat_central = np.mean(\n",
    "    [i.pseudodata.reindex(preds.index.to_list()).to_numpy().flatten() for i in pseudodata],\n",
    "    axis=0,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d373edbc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute delta_T_tilde (Eq. 3.37) and P_tilde (Eq. 3.38) of arXiv:2105.05114\n",
    "\n",
    "# The factors 1/sqrt(2) are to normalize for the fact that beta provides information about\n",
    "# theoretical uncertainties along two directions\n",
    "# CHECK THIS PART\n",
    "# b_tilde SHOULD BE INDEPENDENT OF THE PRIOR THAT WE USE TO MODEL HT CORRECTIONS.\n",
    "\n",
    "central_ht_coeffs = np.zeros(len(H2_coeff_list) + len(HL_coeff_list)) \n",
    "\n",
    "# Construct beta tilde\n",
    "H_single_list = np.concatenate((H2_coeff_list, HL_coeff_list))\n",
    "beta_tilde = []\n",
    "for i, par in enumerate(H_single_list):\n",
    "  aux = np.zeros(H_single_list.size)\n",
    "  aux[i] = par\n",
    "  beta_tilde.append(aux)\n",
    "\n",
    "S_tilde = np.zeros((len(beta_tilde[0]), len(beta_tilde[0])))\n",
    "for tilde in beta_tilde:\n",
    "    S_tilde += np.outer(tilde,tilde)\n",
    "\n",
    "beta = delta_pred\n",
    "S_hat = np.zeros((len(beta_tilde[0]),delta_pred[0].size))\n",
    "for b in zip(beta_tilde, beta):\n",
    "    S_hat += np.outer(b[0], b[1])\n",
    "\n",
    "invcov = np.linalg.inv(C + S)\n",
    "\n",
    "delta_T_tilde = -S_hat @ invcov @ (mean_prediction - dat_central)\n",
    "# where are the X_tilde and X_hat terms in P_tilde?\n",
    "# Maybe not present because we don't have correlations between theory parameters\n",
    "P_tilde = S_hat @ invcov @ X @ invcov @ S_hat.T + (S_tilde - S_hat @ invcov @ S_hat.T)\n",
    "preds = central_ht_coeffs + delta_T_tilde\n",
    "uncs = np.sqrt(P_tilde)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f50ade72",
   "metadata": {},
   "outputs": [],
   "source": [
    "# check if the stored covmat is equal to S we recomputed above\n",
    "fitpath = API.fit(fit=fitname).path\n",
    "try:\n",
    "    stored_covmat = pd.read_csv(\n",
    "        fitpath / \"tables/datacuts_theory_theorycovmatconfig_user_covmat.csv\",\n",
    "        sep=\"\\t\",\n",
    "        encoding=\"utf-8\",\n",
    "        index_col=2,\n",
    "        header=3,\n",
    "        skip_blank_lines=False,\n",
    "    )\n",
    "except FileNotFoundError:\n",
    "    stored_covmat = pd.read_csv(\n",
    "        fitpath / \"tables/datacuts_theory_theorycovmatconfig_theory_covmat_custom.csv\",\n",
    "        index_col=[0, 1, 2],\n",
    "        header=[0, 1, 2],\n",
    "        sep=\"\\t|,\",\n",
    "        engine=\"python\",\n",
    "    ).fillna(0)\n",
    "    storedcovmat_index = pd.MultiIndex.from_tuples(\n",
    "        [(aa, bb, np.int64(cc)) for aa, bb, cc in stored_covmat.index],\n",
    "        names=[\"group\", \"dataset\", \"id\"],\n",
    "    )\n",
    "    stored_covmat = pd.DataFrame(\n",
    "        stored_covmat.values, index=storedcovmat_index, columns=storedcovmat_index\n",
    "    )\n",
    "    stored_covmat = stored_covmat.reindex(S.index).T.reindex(S.index)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "523f127f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# print the final result\n",
    "if np.allclose(S, stored_covmat):\n",
    "    print(\n",
    "        f\"Reversed 5pt\\n\"\n",
    "        f\"-----------------------------\\n\"\n",
    "        )\n",
    "    for i, pred in enumerate(preds):\n",
    "        tpye = \"2\" if i < len(H2_coeff_list) else \"L\"\n",
    "        n = i%7\n",
    "        print(\n",
    "            f\"H_{tpye} node {n+1} = {preds[i]:.5f} ± {np.sqrt(P_tilde[i,i]):.5f} \\n\"\n",
    "        )\n",
    "        if i == len(H2_coeff_list)-1:\n",
    "            print(\"-----------------------------\\n\")\n",
    "else:\n",
    "    print(\"Reconstructed theory covmat, S, is not the same as the stored covmat!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0c2d826",
   "metadata": {},
   "source": [
    "# Plots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31453642",
   "metadata": {},
   "outputs": [],
   "source": [
    "y2_central = [preds[i] for i in range(len(H2_coeff_list))]\n",
    "y2_sigma = [np.sqrt(P_tilde[i,i]) for i in range(len(H2_coeff_list))]\n",
    "y2_plus = [x1 + x2 for x1,x2 in zip(y2_central, y2_sigma)]\n",
    "y2_minus = [x1 - x2 for x1,x2 in zip(y2_central, y2_sigma)]\n",
    "\n",
    "yL_central = [preds[i] for i in range(len(H2_coeff_list), len(HL_coeff_list) + len(H2_coeff_list))]\n",
    "yL_sigma = [np.sqrt(P_tilde[i,i]) for i in range(len(H2_coeff_list), len(HL_coeff_list) + len(H2_coeff_list))]\n",
    "yL_plus = [x1 + x2 for x1,x2 in zip(yL_central, yL_sigma)]\n",
    "yL_minus = [x1 - x2 for x1,x2 in zip(yL_central, yL_sigma)]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16f21817",
   "metadata": {},
   "outputs": [],
   "source": [
    "H2 = sp.interpolate.CubicSpline(x_abmp, y2_central)\n",
    "H2_plus = sp.interpolate.CubicSpline(x_abmp, y2_plus)\n",
    "H2_minus = sp.interpolate.CubicSpline(x_abmp, y2_minus)\n",
    "H2_color = \"sandybrown\"\n",
    "H2_label = \"H2\"\n",
    "\n",
    "\n",
    "HL = sp.interpolate.CubicSpline(x_abmp, yL_central)\n",
    "HL_plus = sp.interpolate.CubicSpline(x_abmp, yL_plus)\n",
    "HL_minus = sp.interpolate.CubicSpline(x_abmp, yL_minus)\n",
    "HL_color = \"green\"\n",
    "HL_label = \"HL\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "00a1aa80",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_wrapper(H, H_p, H_m, y, label, color, ylabel):\n",
    "  xv = np.logspace(-5, -0.0001, 100)\n",
    "  legends = []\n",
    "  legend_name = [label, \"knots\"]\n",
    "  fig, ax = plt.subplots(figsize=(12.5, 8))\n",
    "  knots = ax.plot(x_abmp, y, 'o', label='data')\n",
    "  pl = ax.plot(xv, H(xv), ls = \"-\", lw = 1, color=color)\n",
    "  pl_lg= ax.fill(np.NaN, np.NaN, color = color, alpha = 0.3) # Necessary for fancy legend\n",
    "  legends.append((pl[0], pl_lg[0]))\n",
    "  legends.append(knots[0])\n",
    "  ax.fill_between(xv, H_p(xv), H_m(xv), color = color, alpha = 0.3)\n",
    "  ax.set_xscale(\"log\")\n",
    "  ax.set_xlabel(f'$x$')\n",
    "  ax.set_ylabel(ylabel)\n",
    "  \n",
    "  \n",
    "  fig.legend(legends, legend_name, loc=[0.1,0.15], fontsize=15)\n",
    "  plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd78dc44",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_wrapper(H2, H2_plus, H2_minus, y2_central, label=r\"$H_2 \\pm \\sigma$\", color=H2_color, ylabel=r\"$H_2$\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "393c4847",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_wrapper(HL, HL_plus, HL_minus, yL_central, label=r\"$H_L \\pm \\sigma$\", color=HL_color, ylabel=r\"$H_L$\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axs = plt.subplots(1,2, figsize=(15, 7))\n",
    "H2_dict = {\n",
    "  \"func\" : H2,\n",
    "  \"func_plus_std\": H2_plus,\n",
    "  \"func_minus_std\": H2_minus,\n",
    "  \"knots\": y2_central,\n",
    "  \"label\": r\"$H_2 \\pm \\sigma$\",\n",
    "  \"ylabel\": r\"$H_2$\",\n",
    "  \"color\": H2_color\n",
    "}\n",
    "\n",
    "HL_dict = {\n",
    "  \"func\" : HL,\n",
    "  \"func_plus_std\": HL_plus,\n",
    "  \"func_minus_std\": HL_minus,\n",
    "  \"knots\": yL_central,\n",
    "  \"label\": r\"$H_L \\pm \\sigma$\",\n",
    "  \"ylabel\": r\"$H_L$\",\n",
    "  \"color\": HL_color\n",
    "}\n",
    "dicts = [H2_dict, HL_dict]\n",
    "\n",
    "def plot_wrapper(H, H_p, H_m, y, label, color, ylabel, ax):\n",
    "  xv = np.logspace(-5, -0.0001, 100)\n",
    "  legends = []\n",
    "  legend_name = [label, \"knots\"]\n",
    "  knots = ax.plot(x_abmp, y, 'o', label='data')\n",
    "  pl = ax.plot(xv, H(xv), ls = \"-\", lw = 1, color=color)\n",
    "  pl_lg= ax.fill(np.NaN, np.NaN, color = color, alpha = 0.3) # Necessary for fancy legend\n",
    "  legends.append((pl[0], pl_lg[0]))\n",
    "  legends.append(knots[0])\n",
    "  ax.fill_between(xv, H_p(xv), H_m(xv), color = color, alpha = 0.3)\n",
    "  ax.set_xscale(\"log\")\n",
    "  ax.set_xlabel(f'$x$')\n",
    "  ax.set_ylabel(ylabel)\n",
    "  ax.legend(legends, legend_name, loc=[0.1,0.15], fontsize=15)\n",
    "\n",
    "for i, HTdict in enumerate(dicts):\n",
    "  plot_wrapper(H=HTdict[\"func\"],\n",
    "               H_p=HTdict[\"func_plus_std\"],\n",
    "               H_m=HTdict[\"func_minus_std\"],\n",
    "               y=HTdict[\"knots\"],\n",
    "               label=HTdict[\"label\"],\n",
    "               color=HTdict[\"color\"],\n",
    "               ylabel=HTdict[\"ylabel\"],\n",
    "               ax=axs[i])\n",
    "\n",
    "axs[0].text(5.e-5, 0.06, fitname, fontsize=20)\n",
    "fig.savefig()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d960e863",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nnpdf",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  },
  "vscode": {
   "interpreter": {
    "hash": "589e3134e9d89160e5ace28972e8dc0b682f48816407b59cbfdad217f6fb745b"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "226dc610",
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "from validphys.api import API\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# fitname = \"240108-01-rs-ht-tcm-disonly\" # ht_coeff = 2.5\n",
    "# fitname = \"240109-01-rs-ht-tcm-disonly\" # ht_coeff = 1.0 | -0.02256 ± 0.02550\n",
    "# fitname = \"240109-02-rs-ht-tcm-disonly\" # ht_coeff = 1.0, iterated | -0.03457 ± 0.02597\n",
    "# fitname = \"240109-03-rs-ht-tcm-disonly\" # ht_coeff = 2.5, iterated | 0.61057 ± 0.02822\n",
    "\n",
    "# fitname = \"240109-04-rs-ht-tcm-disonly\" # 240109-02-rs-ht-tcm-disonly with lowered cuts | 0.21670 ± 0.01147\n",
    "# fitname = \"240110-01-rs-ht-tcm-disonly\" # 240109-02-rs-ht-tcm-disonly with highered cuts | 0.05157 ± 0.07621\n",
    "\n",
    "# fitname = \"240112-01-ak-ht-tcm-disonly\" # ht_coeff = 1.0 | n. replicas = 500 | -0.02534 ± 0.02577\n",
    "# fitname = \"240122-02-ak-ht-tcm-disonly\" # ht_coeff = 1.2 | n. replicas = 500 |  0.00798 ± 0.02604\n",
    "# fitname = \"240122-03-ak-ht-tcm-disonly\" # ht_coeff = 1.5 | n. replicas = 500 |  0.07592 ± 0.02578 \n",
    "# fitname = \"240122-04-ak-ht-tcm-disonly\" # ht_coeff = 1.7 | n. replicas = 500 |  0.14147 ± 0.02690 \n",
    "# fitname = \"240122-05-ak-ht-tcm-disonly\" # ht_coeff = 2.0 | n. replicas = 500 |  0.28917 ± 0.02722 \n",
    "# Comments: \n",
    "#  - The uncertainty seems to be independent on the prior (but might depend on the cuts and the number of replicas).\n",
    "#  - The central values is unstable, and should not depend on the the prior, whereas it does.\n",
    "\n",
    "# fitname = \"240129-01-ak-ht-tcm-disonly\" # ht_coeff = 1.0 | n. replicas = 400 | 0.41233 ± 0.00744 lowered cuts (1, 1)\n",
    "# fitname = \"240129-02-ak-ht-tcm-disonly\" # ht_coeff = 1.0 | n. replicas = 500 | 0.21803 ± 0.01103 lowered cuts (1.49, 6.5)\n",
    "# fitname = \"240129-03-ak-ht-tcm-disonly\" # ht_coeff = 1.0 | n. replicas = 500 | 0.38203 ± 0.13429 risen cuts (20,60)\n",
    "\n",
    "\n",
    "# fitname = \"240205-01-ach-ht-tcm-disonly\"  #    (10, 1)     | h1 = 6.33299 ± 0.05363  h2 = 0.63330 ± 0.00536 std cuts\n",
    "# fitname = \"240205-02-ach-ht-tcm-disonly\"  #    (10, 1) |     | h1 = 7.49233 ± 0.01588  h2 = 0.74923 ± 0.00159 low cuts (1,1)\n",
    "# fitname = \"240205-03-ach-ht-tcm-disonly\"  #    (10, 1) |     | h1 = 1.82355 ± 0.162934  h2 = 0.18235 ± 0.01629 high cuts (20, 60)\n",
    "# fitname = \"240206-01-ach-ht-tcm-disonly\"  #    (1, 10) |     | h1 = 0.33709 ± 0.00869  h2 = 3.37087 ± 0.08688 std cuts\n",
    "# fitname = \"240206-02-ach-ht-tcm-disonly\"  #    (1, 10) |     | h1 = 0.60335 ± 0.00457  h2 = 6.03352 ± 0.04567 low cuts (1, 1)\n",
    "# fitname = \"240206-03-ach-ht-tcm-disonly\"  #    (1, 10) |     | h1 = 0.17730 ± 0.04032  h2 = 1.77300 ± 0.40317 high cuts (20, 60)\n",
    "# fitname = \"240208-03-ach-ht-tcm-disonly\"  #    (1, 10) |     | h1 = 0.16821 ± 0.01151  h2 = 1.68213 ± 0.11506 high cuts (5, 20)\n",
    "# fitname = \"240208-04-ach-ht-tcm-disonly\"  #    (10, 1) |     | h1 = 0.33709 ± 0.00869  h2 = 3.37087 ± 0.08688 high cuts (20, 60)\n",
    "\n",
    "fitnames = {}\n",
    "#fitnames[\"fitname_1\"]  = \"240220-01-ach-ht-tcm\"  #   (10, 10)  | std. cuts | h1 = 0.33709 ± 0.00869  h2 = 3.37087 ± 0.08688\n",
    "#fitnames[\"fitname_2\"]  = \"240220-02-ach-ht-tcm\"  #   (10, 10)  | low cuts  | h1 = 0.33709 ± 0.00869  h2 = 3.37087 ± 0.08688\n",
    "#fitnames[\"fitname_3\"]  = \"240220-03-ach-ht-tcm\"  #   (10, 10)  | high cuts | h1 = 0.33709 ± 0.00869  h2 = 3.37087 ± 0.08688\n",
    "#fitnames[\"fitname_4\"]  = \"240220-04-ach-ht-tcm\"  #   (1, 10)   | std. cuts | h1 = 0.33709 ± 0.00869  h2 = 3.37087 ± 0.08688\n",
    "#fitnames[\"fitname_5\"]  = \"240220-05-ach-ht-tcm\"  #   (1, 10)   | low cuts  | h1 = 0.33709 ± 0.00869  h2 = 3.37087 ± 0.08688\n",
    "#fitnames[\"fitname_6\"]  = \"240220-06-ach-ht-tcm\"  #   (1, 10)   | high cuts | h1 = 0.33709 ± 0.00869  h2 = 3.37087 ± 0.08688\n",
    "#fitnames[\"fitname_7\"]  = \"240220-07-ach-ht-tcm\"  #   (10, 1)   | std. cuts | h1 = 0.33709 ± 0.00869  h2 = 3.37087 ± 0.08688\n",
    "#fitnames[\"fitname_8\"]  = \"240220-08-ach-ht-tcm\"  #   (10, 1)   | low cuts  | h1 = 0.33709 ± 0.00869  h2 = 3.37087 ± 0.08688\n",
    "#fitnames[\"fitname_9\"]  = \"240220-09-ach-ht-tcm\"  #   (10, 1)   | high cuts | h1 = 0.33709 ± 0.00869  h2 = 3.37087 ± 0.08688\n",
    "#fitnames[\"fitname_10\"] = \"240220-10-ach-ht-tcm\"  #   (1, 1)    | std. cuts | h1 = 0.33709 ± 0.00869  h2 = 3.37087 ± 0.08688\n",
    "#fitnames[\"fitname_11\"] = \"240220-11-ach-ht-tcm\"  #   (1, 1)    | low cuts  | h1 = 0.33709 ± 0.00869  h2 = 3.37087 ± 0.08688\n",
    "#fitnames[\"fitname_12\"] = \"240220-12-ach-ht-tcm\"  #   (1, 1)    | high cuts | h1 = 0.33709 ± 0.00869  h2 = 3.37087 ± 0.08688"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28d8b18f",
   "metadata": {},
   "outputs": [],
   "source": [
    "for fitname in fitnames.items():\n",
    "    print(f\"{fitname[0]} : {fitname[1]}\")\n",
    "    ComputePosterior(fitname[1])\n",
    "    print(f\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92f93410",
   "metadata": {},
   "outputs": [],
   "source": [
    "fitname = \"240308-01-ach-ht-5pt-nom\"\n",
    "thcovmat_dict = API.fit(fit=fitname).as_input()[\"theorycovmatconfig\"]\n",
    "\n",
    "if \"ht_version\" in thcovmat_dict:\n",
    "    version = thcovmat_dict[\"ht_version\"]\n",
    "else:\n",
    "    version = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7dae95b",
   "metadata": {},
   "outputs": [],
   "source": [
    "if version == 1:\n",
    "    ht_coeff = thcovmat_dict[\"ht_coeff\"]\n",
    "elif version == 2 or version == 3:\n",
    "    ht_coeff_1 = thcovmat_dict[\"ht_coeff_1\"]\n",
    "    ht_coeff_2 = thcovmat_dict[\"ht_coeff_2\"]\n",
    "\n",
    "# dict used to produce theory predictions to construct the theory covmat as well as to produce\n",
    "# theory predictions from the fit performed using the ht covmat (i.e. the predicitons that should\n",
    "# be compared to data)\n",
    "common_dict = dict(\n",
    "    dataset_inputs={\"from_\": \"fit\"},\n",
    "    fit=fitname,\n",
    "    fits=[fitname],\n",
    "    use_cuts=\"fromfit\",\n",
    "    metadata_group=\"nnpdf31_process\",\n",
    "    theory={\"from_\": \"fit\"},\n",
    "    theoryid={\"from_\": \"theory\"},\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08d0d9a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# collect the information (predictions + kinematics) needed for the computation of the HT covmat\n",
    "\n",
    "# Calculate theory predictions of the input PDF\n",
    "S_dict = dict(\n",
    "    theorycovmatconfig={\"from_\": \"fit\"},\n",
    "    pdf={\"from_\": \"theorycovmatconfig\"},\n",
    "    use_t0=True,\n",
    "    datacuts={\"from_\": \"fit\"},\n",
    "    t0pdfset={\"from_\": \"datacuts\"},\n",
    ")\n",
    "preds_ht_cov_construction = API.group_result_table_no_table(**(S_dict | common_dict))\n",
    "\n",
    "# collect the corresponding kinemacs\n",
    "process_info = API.combine_by_type_ht(**(S_dict | common_dict))\n",
    "N_full_data = np.sum([i for i in process_info.sizes.values()])\n",
    "kinematics_DIS = np.concatenate([v for v in [process_info.data[\"DIS NC\"], process_info.data[\"DIS CC\"]]]).T\n",
    "# TO CHECK: IS preds[][1] THE THEORY PREDICTION?\n",
    "preds_DIS = np.concatenate([v for v in [process_info.preds[\"DIS NC\"][1], process_info.preds[\"DIS CC\"][1]]]).T\n",
    "xvals_DIS = kinematics_DIS[0]\n",
    "q2vals_DIS = kinematics_DIS[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c21605b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate theory predictions of the fit with ht covmat - this will be compared to data\n",
    "preds = API.group_result_table_no_table(pdf={\"from_\": \"fit\"}, **common_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc820c3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# compute the matrix X encoding the PDF uncertainties of the predictions\n",
    "preds_onlyreplicas = preds.iloc[:, 2:].to_numpy()\n",
    "mean_prediction = np.mean(preds_onlyreplicas, axis=1)\n",
    "\n",
    "X = np.zeros((preds.shape[0], preds.shape[0]))\n",
    "for i in range(preds_onlyreplicas.shape[1]):\n",
    "    X += np.outer(\n",
    "        (preds_onlyreplicas[:, i] - mean_prediction), (preds_onlyreplicas[:, i] - mean_prediction)\n",
    "    )\n",
    "X *= 1 / preds_onlyreplicas.shape[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76acc02c",
   "metadata": {},
   "outputs": [],
   "source": [
    "preds_ht = pd.DataFrame(preds_ht_cov_construction['theory_central'])\n",
    "# compute the delta of the theory prediction\n",
    "if version == 1:\n",
    "    preds_ht[\"higher twist\"] = 0\n",
    "    preds_ht.loc[['DIS NC', 'DIS CC'],'higher twist'] = ht_coeff * (\n",
    "        preds_ht.loc[['DIS NC', 'DIS CC'], 'theory_central'] / q2vals_DIS/ (1 - xvals_DIS)\n",
    "    )\n",
    "elif version == 2:\n",
    "    preds_ht[\"higher twist\"] = 0\n",
    "    preds_ht.loc[['DIS NC', 'DIS CC'],'higher twist'] = preds_ht.loc[['DIS NC', 'DIS CC'], 'theory_central'] * ( \n",
    "         ht_coeff_1 + ht_coeff_2 * xvals_DIS / (1 - xvals_DIS)\n",
    "    ) / q2vals_DIS\n",
    "elif version == 3:\n",
    "    preds_ht[\"higher twist (±,0)\"] = 0\n",
    "    preds_ht[\"higher twist (0,±)\"] = 0\n",
    "    # Compute beta for shift (±,0)\n",
    "    preds_ht.loc[['DIS NC', 'DIS CC'],'higher twist (±,0)'] = preds_ht.loc[['DIS NC', 'DIS CC'], 'theory_central'] * (\n",
    "        ht_coeff_1 / q2vals_DIS\n",
    "    )\n",
    "    preds_ht.loc[['DIS NC', 'DIS CC'],'higher twist (0,±)'] = preds_ht.loc[['DIS NC', 'DIS CC'], 'theory_central'] * (\n",
    "        ht_coeff_2 * xvals_DIS / (1 - xvals_DIS) / q2vals_DIS\n",
    "        )\n",
    "\n",
    "#delta_pred = preds_ht['higher twist']\n",
    "delta_pred = []\n",
    "\n",
    "if version == 1 or version == 2:\n",
    "    delta_pred.append(preds_ht['higher twist'])\n",
    "elif version == 3:\n",
    "    delta_pred.append(preds_ht['higher twist (±,0)'])\n",
    "    delta_pred.append(preds_ht['higher twist (0,±)'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "800cd4de",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Theory covariance matrix\n",
    "S = np.zeros((delta_pred[0].size, delta_pred[0].size))\n",
    "for delta in delta_pred:\n",
    "    S += np.outer(delta, delta)\n",
    "\n",
    "S = pd.DataFrame(S, index=delta_pred[0].index, columns=delta_pred[0].index)\n",
    "\n",
    "# Experimental covariance matrix\n",
    "C = API.groups_covmat_no_table(**common_dict)\n",
    "\n",
    "# Ensure that S anc C are ordered in the same way (in practice they already are)\n",
    "S = S.reindex(C.index).T.reindex(C.index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d896cf55",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the central value of the pseudodata\n",
    "# this is needed to compute the distance between prediction and data\n",
    "pseudodata = API.read_pdf_pseudodata(**common_dict)\n",
    "dat_central = np.mean(\n",
    "    [i.pseudodata.reindex(preds.index.to_list()).to_numpy().flatten() for i in pseudodata],\n",
    "    axis=0,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d373edbc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute delta_T_tilde (Eq. 3.37) and P_tilde (Eq. 3.38) of arXiv:2105.05114\n",
    "\n",
    "# The factors 1/sqrt(2) are to normalize for the fact that beta provides information about\n",
    "# theoretical uncertainties along two directions\n",
    "# CHECK THIS PART\n",
    "# b_tilde SHOULD BE INDEPENDENT OF THE PRIOR THAT WE USE TO MODEL HT CORRECTIONS.\n",
    "if version == 1:\n",
    "    central_ht_coeffs = [0] # central prediction for ht_coeff\n",
    "    beta_tilde = [[ht_coeff]]\n",
    "\n",
    "elif version == 2:\n",
    "    central_ht_coeffs = [0, 0] # central prediction for ht_coeff_1 and ht_coeff_2\n",
    "    beta_tilde = [[ht_coeff_1, ht_coeff_2]]\n",
    "\n",
    "elif version == 3:\n",
    "    central_ht_coeffs = [0, 0] # central prediction for ht_coeff_1 and ht_coeff_2\n",
    "    beta_tilde = [[ht_coeff_1, 0], [0, ht_coeff_2]]\n",
    "\n",
    "S_tilde = np.zeros((len(beta_tilde[0]), len(beta_tilde[0])))\n",
    "for tilde in beta_tilde:\n",
    "    S_tilde += np.outer(tilde,tilde)\n",
    "\n",
    "beta = delta_pred\n",
    "S_hat = np.zeros((len(beta_tilde[0]),delta_pred[0].size))\n",
    "for b in zip(beta_tilde, beta):\n",
    "    S_hat += np.outer(b[0], b[1])\n",
    "\n",
    "invcov = np.linalg.inv(C + S)\n",
    "\n",
    "delta_T_tilde = -S_hat @ invcov @ (mean_prediction - dat_central)\n",
    "# where are the X_tilde and X_hat terms in P_tilde?\n",
    "# Maybe not present because we don't have correlations between theory parameters\n",
    "P_tilde = S_hat @ invcov @ X @ invcov @ S_hat.T + (S_tilde - S_hat @ invcov @ S_hat.T)\n",
    "preds = central_ht_coeffs + delta_T_tilde\n",
    "uncs = np.sqrt(P_tilde)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f50ade72",
   "metadata": {},
   "outputs": [],
   "source": [
    "# check if the stored covmat is equal to S we recomputed above\n",
    "fitpath = API.fit(fit=fitname).path\n",
    "try:\n",
    "    stored_covmat = pd.read_csv(\n",
    "        fitpath / \"tables/datacuts_theory_theorycovmatconfig_user_covmat.csv\",\n",
    "        sep=\"\\t\",\n",
    "        encoding=\"utf-8\",\n",
    "        index_col=2,\n",
    "        header=3,\n",
    "        skip_blank_lines=False,\n",
    "    )\n",
    "except FileNotFoundError:\n",
    "    stored_covmat = pd.read_csv(\n",
    "        fitpath / \"tables/datacuts_theory_theorycovmatconfig_theory_covmat_custom.csv\",\n",
    "        index_col=[0, 1, 2],\n",
    "        header=[0, 1, 2],\n",
    "        sep=\"\\t|,\",\n",
    "        engine=\"python\",\n",
    "    ).fillna(0)\n",
    "    storedcovmat_index = pd.MultiIndex.from_tuples(\n",
    "        [(aa, bb, np.int64(cc)) for aa, bb, cc in stored_covmat.index],\n",
    "        names=[\"group\", \"dataset\", \"id\"],\n",
    "    )\n",
    "    stored_covmat = pd.DataFrame(\n",
    "        stored_covmat.values, index=storedcovmat_index, columns=storedcovmat_index\n",
    "    )\n",
    "    stored_covmat = stored_covmat.reindex(S.index).T.reindex(S.index)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "523f127f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# print the final result\n",
    "if np.allclose(S, stored_covmat):\n",
    "    if version == 1:\n",
    "        print(\n",
    "            f\"Prediction for ht_coeff: {preds[0]:.5f} ± {uncs[0,0]:.5f} \\n\"\n",
    "            f\"ht_coeff : {ht_coeff}\"\n",
    "            )\n",
    "    elif version == 2:\n",
    "        print(\n",
    "            f\"Prediction for \\n ht_coeff_1: {preds[0]:.5f} ± {uncs[0,0]:.5f} \\n ht_coeff_2: {preds[1]:.5f} ± {uncs[1,1]:.5f}\"\n",
    "            )\n",
    "else:\n",
    "    print(\"Reconstructed theory covmat, S, is not the same as the stored covmat!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ec79a12",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.colors as mcolors\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(9, 6))\n",
    "ax.scatter(\n",
    "    xvals,\n",
    "    q2vals,\n",
    "    marker=\".\",\n",
    "    c=stored_covmat.to_numpy().diagonal(),\n",
    "    cmap=\"viridis\",\n",
    "    norm=mcolors.LogNorm(),\n",
    ")\n",
    "ax.set_xscale(\"log\")\n",
    "ax.set_yscale(\"log\")\n",
    "ax.set_xlabel(r\"$x$\")\n",
    "ax.set_ylabel(r\"$Q^2$\")\n",
    "\n",
    "filterlist = []\n",
    "q2min = 5\n",
    "w2min = 20\n",
    "for i,(x,q2) in enumerate(zip(xvals, q2vals)):\n",
    "    w2 = q2*(1-x)/x\n",
    "    if q2 < q2min:\n",
    "        filterlist.append(i)\n",
    "    elif w2 < w2min:\n",
    "        filterlist.append(i)\n",
    "\n",
    "ax.scatter(\n",
    "    xvals[filterlist],\n",
    "    q2vals[filterlist],\n",
    "    marker=\".\",\n",
    "    color=\"black\"\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63e11d00",
   "metadata": {},
   "outputs": [],
   "source": [
    "API.plot_xq2(\n",
    "    fit=fitname,\n",
    "    dataset_inputs={\"from_\": \"fit\"},\n",
    "    use_cuts=\"fromfit\",\n",
    "    display_cuts=False,\n",
    "    marker_by=\"dataset\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa074b58",
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.core.display import display\n",
    "pdfs = [\n",
    "    \"240109-02-rs-ht-tcm-disonly\",\n",
    "    \"240109-03-rs-ht-tcm-disonly\"\n",
    "]\n",
    "%matplotlib widget\n",
    "flavours = ['g', 'u', 'd', 'ubar' ]\n",
    "figs = API.plot_pdfs(pdfs=pdfs, Q=1.65, flavours=flavours)\n",
    "for fig, fl in figs:\n",
    "    fig.tight_layout()\n",
    "    display(fig)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nnpdf",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  },
  "vscode": {
   "interpreter": {
    "hash": "589e3134e9d89160e5ace28972e8dc0b682f48816407b59cbfdad217f6fb745b"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

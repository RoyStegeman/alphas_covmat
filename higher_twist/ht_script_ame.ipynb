{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "226dc610",
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "from validphys.api import API\n",
    "import scipy as sp\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# fitname = \"240108-01-rs-ht-tcm-disonly\" # ht_coeff = 2.5\n",
    "# fitname = \"240109-01-rs-ht-tcm-disonly\" # ht_coeff = 1.0 | -0.02256 ± 0.02550\n",
    "# fitname = \"240109-02-rs-ht-tcm-disonly\" # ht_coeff = 1.0, iterated | -0.03457 ± 0.02597\n",
    "# fitname = \"240109-03-rs-ht-tcm-disonly\" # ht_coeff = 2.5, iterated | 0.61057 ± 0.02822\n",
    "\n",
    "# fitname = \"240109-04-rs-ht-tcm-disonly\" # 240109-02-rs-ht-tcm-disonly with lowered cuts | 0.21670 ± 0.01147\n",
    "# fitname = \"240110-01-rs-ht-tcm-disonly\" # 240109-02-rs-ht-tcm-disonly with highered cuts | 0.05157 ± 0.07621\n",
    "\n",
    "# fitname = \"240112-01-ak-ht-tcm-disonly\" # ht_coeff = 1.0 | n. replicas = 500 | -0.02534 ± 0.02577\n",
    "# fitname = \"240122-02-ak-ht-tcm-disonly\" # ht_coeff = 1.2 | n. replicas = 500 |  0.00798 ± 0.02604\n",
    "# fitname = \"240122-03-ak-ht-tcm-disonly\" # ht_coeff = 1.5 | n. replicas = 500 |  0.07592 ± 0.02578 \n",
    "# fitname = \"240122-04-ak-ht-tcm-disonly\" # ht_coeff = 1.7 | n. replicas = 500 |  0.14147 ± 0.02690 \n",
    "# fitname = \"240122-05-ak-ht-tcm-disonly\" # ht_coeff = 2.0 | n. replicas = 500 |  0.28917 ± 0.02722 \n",
    "# Comments: \n",
    "#  - The uncertainty seems to be independent on the prior (but might depend on the cuts and the number of replicas).\n",
    "#  - The central values is unstable, and should not depend on the the prior, whereas it does.\n",
    "\n",
    "# fitname = \"240129-01-ak-ht-tcm-disonly\" # ht_coeff = 1.0 | n. replicas = 400 | 0.41233 ± 0.00744 lowered cuts (1, 1)\n",
    "# fitname = \"240129-02-ak-ht-tcm-disonly\" # ht_coeff = 1.0 | n. replicas = 500 | 0.21803 ± 0.01103 lowered cuts (1.49, 6.5)\n",
    "# fitname = \"240129-03-ak-ht-tcm-disonly\" # ht_coeff = 1.0 | n. replicas = 500 | 0.38203 ± 0.13429 risen cuts (20,60)\n",
    "\n",
    "\n",
    "# fitname = \"240205-01-ach-ht-tcm-disonly\"  #    (10, 1)     | h1 = 6.33299 ± 0.05363  h2 = 0.63330 ± 0.00536 std cuts\n",
    "# fitname = \"240205-02-ach-ht-tcm-disonly\"  #    (10, 1) |     | h1 = 7.49233 ± 0.01588  h2 = 0.74923 ± 0.00159 low cuts (1,1)\n",
    "# fitname = \"240205-03-ach-ht-tcm-disonly\"  #    (10, 1) |     | h1 = 1.82355 ± 0.162934  h2 = 0.18235 ± 0.01629 high cuts (20, 60)\n",
    "# fitname = \"240206-01-ach-ht-tcm-disonly\"  #    (1, 10) |     | h1 = 0.33709 ± 0.00869  h2 = 3.37087 ± 0.08688 std cuts\n",
    "# fitname = \"240206-02-ach-ht-tcm-disonly\"  #    (1, 10) |     | h1 = 0.60335 ± 0.00457  h2 = 6.03352 ± 0.04567 low cuts (1, 1)\n",
    "# fitname = \"240206-03-ach-ht-tcm-disonly\"  #    (1, 10) |     | h1 = 0.17730 ± 0.04032  h2 = 1.77300 ± 0.40317 high cuts (20, 60)\n",
    "# fitname = \"240208-03-ach-ht-tcm-disonly\"  #    (1, 10) |     | h1 = 0.16821 ± 0.01151  h2 = 1.68213 ± 0.11506 high cuts (5, 20)\n",
    "# fitname = \"240208-04-ach-ht-tcm-disonly\"  #    (10, 1) |     | h1 = 0.33709 ± 0.00869  h2 = 3.37087 ± 0.08688 high cuts (20, 60)\n",
    "\n",
    "fitnames = {}\n",
    "#fitnames[\"fitname_1\"]  = \"240220-01-ach-ht-tcm\"  #   (10, 10)  | std. cuts | h1 = 0.33709 ± 0.00869  h2 = 3.37087 ± 0.08688\n",
    "#fitnames[\"fitname_2\"]  = \"240220-02-ach-ht-tcm\"  #   (10, 10)  | low cuts  | h1 = 0.33709 ± 0.00869  h2 = 3.37087 ± 0.08688\n",
    "#fitnames[\"fitname_3\"]  = \"240220-03-ach-ht-tcm\"  #   (10, 10)  | high cuts | h1 = 0.33709 ± 0.00869  h2 = 3.37087 ± 0.08688\n",
    "#fitnames[\"fitname_4\"]  = \"240220-04-ach-ht-tcm\"  #   (1, 10)   | std. cuts | h1 = 0.33709 ± 0.00869  h2 = 3.37087 ± 0.08688\n",
    "#fitnames[\"fitname_5\"]  = \"240220-05-ach-ht-tcm\"  #   (1, 10)   | low cuts  | h1 = 0.33709 ± 0.00869  h2 = 3.37087 ± 0.08688\n",
    "#fitnames[\"fitname_6\"]  = \"240220-06-ach-ht-tcm\"  #   (1, 10)   | high cuts | h1 = 0.33709 ± 0.00869  h2 = 3.37087 ± 0.08688\n",
    "#fitnames[\"fitname_7\"]  = \"240220-07-ach-ht-tcm\"  #   (10, 1)   | std. cuts | h1 = 0.33709 ± 0.00869  h2 = 3.37087 ± 0.08688\n",
    "#fitnames[\"fitname_8\"]  = \"240220-08-ach-ht-tcm\"  #   (10, 1)   | low cuts  | h1 = 0.33709 ± 0.00869  h2 = 3.37087 ± 0.08688\n",
    "#fitnames[\"fitname_9\"]  = \"240220-09-ach-ht-tcm\"  #   (10, 1)   | high cuts | h1 = 0.33709 ± 0.00869  h2 = 3.37087 ± 0.08688\n",
    "#fitnames[\"fitname_10\"] = \"240220-10-ach-ht-tcm\"  #   (1, 1)    | std. cuts | h1 = 0.33709 ± 0.00869  h2 = 3.37087 ± 0.08688\n",
    "#fitnames[\"fitname_11\"] = \"240220-11-ach-ht-tcm\"  #   (1, 1)    | low cuts  | h1 = 0.33709 ± 0.00869  h2 = 3.37087 ± 0.08688\n",
    "#fitnames[\"fitname_12\"] = \"240220-12-ach-ht-tcm\"  #   (1, 1)    | high cuts | h1 = 0.33709 ± 0.00869  h2 = 3.37087 ± 0.08688"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "92f93410",
   "metadata": {},
   "outputs": [],
   "source": [
    "fitname = \"240412-01-ach-ht-5pt-ABMP\"\n",
    "thcovmat_dict = API.fit(fit=fitname).as_input()[\"theorycovmatconfig\"]\n",
    "\n",
    "if \"ht_version\" in thcovmat_dict:\n",
    "    version = thcovmat_dict[\"ht_version\"]\n",
    "else:\n",
    "    version = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d7dae95b",
   "metadata": {},
   "outputs": [],
   "source": [
    "if version == 1:\n",
    "    ht_coeff = thcovmat_dict[\"ht_coeff\"]\n",
    "elif version == 2 or version == 3 or version == 4:\n",
    "    ht_coeff_1 = thcovmat_dict[\"ht_coeff_1\"]\n",
    "    ht_coeff_2 = thcovmat_dict[\"ht_coeff_2\"]\n",
    "\n",
    "\n",
    "# dict used to produce theory predictions to construct the theory covmat as well as to produce\n",
    "# theory predictions from the fit performed using the ht covmat (i.e. the predicitons that should\n",
    "# be compared to data)\n",
    "common_dict = dict(\n",
    "    dataset_inputs={\"from_\": \"fit\"},\n",
    "    fit=fitname,\n",
    "    fits=[fitname],\n",
    "    use_cuts=\"fromfit\",\n",
    "    metadata_group=\"nnpdf31_process\",\n",
    "    theory={\"from_\": \"fit\"},\n",
    "    theoryid={\"from_\": \"theory\"},\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "08d0d9a3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LHAPDF 6.5.4 loading /opt/homebrew/Caskroom/miniconda/base/envs/nnpdf/share/LHAPDF/210715-n3fit-1000-001/210715-n3fit-1000-001_0000.dat\n",
      "210715-n3fit-1000-001 PDF set, member #0, version 1\n",
      "LHAPDF 6.5.4 loading /opt/homebrew/Caskroom/miniconda/base/envs/nnpdf/share/LHAPDF/210619-n3fit-001/210619-n3fit-001_0000.dat\n",
      "210619-n3fit-001 PDF set, member #0, version 1\n",
      "LHAPDF 6.5.4 loading all 101 PDFs in set 210619-n3fit-001\n",
      "210619-n3fit-001, version 1; 101 PDF members\n"
     ]
    }
   ],
   "source": [
    "# collect the information (predictions + kinematics) needed for the computation of the HT covmat\n",
    "\n",
    "# Calculate theory predictions of the input PDF\n",
    "S_dict = dict(\n",
    "    theorycovmatconfig={\"from_\": \"fit\"},\n",
    "    pdf={\"from_\": \"theorycovmatconfig\"},\n",
    "    use_t0=True,\n",
    "    datacuts={\"from_\": \"fit\"},\n",
    "    t0pdfset={\"from_\": \"datacuts\"},\n",
    ")\n",
    "preds_ht_cov_construction = API.group_result_central_table_no_table(**(S_dict | common_dict))\n",
    "preds_ht = pd.DataFrame(preds_ht_cov_construction['theory_central'])\n",
    "\n",
    "# collect the corresponding kinemacs\n",
    "process_info = API.combine_by_type_ht(**(S_dict | common_dict))\n",
    "N_full_data = np.sum([i for i in process_info.sizes.values()])\n",
    "kinematics_DIS = np.concatenate([v for v in [process_info.data[\"DIS NC\"], process_info.data[\"DIS CC\"]]]).T\n",
    "# TO CHECK: IS preds[][1] THE THEORY PREDICTION?\n",
    "preds_DIS = np.concatenate([v for v in [process_info.preds[\"DIS NC\"][1], process_info.preds[\"DIS CC\"][1]]]).T\n",
    "xvals_DIS = kinematics_DIS[0]\n",
    "q2vals_DIS = kinematics_DIS[1]\n",
    "yvals_DIS = kinematics_DIS[2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "9c21605b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LHAPDF 6.5.4 loading all 501 PDFs in set 240412-01-ach-ht-5pt-ABMP\n",
      "240412-01-ach-ht-5pt-ABMP, version 1; 501 PDF members\n"
     ]
    }
   ],
   "source": [
    "# Calculate theory predictions of the fit with ht covmat - this will be compared to data\n",
    "preds = API.group_result_table_no_table(pdf={\"from_\": \"fit\"}, **common_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "bc820c3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# compute the matrix X encoding the PDF uncertainties of the predictions\n",
    "preds_onlyreplicas = preds.iloc[:, 2:].to_numpy()\n",
    "mean_prediction = np.mean(preds_onlyreplicas, axis=1)\n",
    "\n",
    "X = np.zeros((preds.shape[0], preds.shape[0]))\n",
    "for i in range(preds_onlyreplicas.shape[1]):\n",
    "    X += np.outer(\n",
    "        (preds_onlyreplicas[:, i] - mean_prediction), (preds_onlyreplicas[:, i] - mean_prediction)\n",
    "    )\n",
    "X *= 1 / preds_onlyreplicas.shape[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "5584ad71",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_normalisation_by_experiment(experiment_name, x, y, Q2):\n",
    "    N_2 = np.zeros(shape=y.shape)\n",
    "    N_L = np.zeros(shape=y.shape)\n",
    "\n",
    "    if \"HERA_NC\" in experiment_name or \"HERA_CC\" in experiment_name or \"NMC\" in experiment_name:\n",
    "        yp = 1 + np.power(1 - y, 2)\n",
    "        yL = np.power(y, 2)\n",
    "\n",
    "        if \"HERA_NC\" in experiment_name or \"NMC\" in experiment_name:\n",
    "            N_2 = 1\n",
    "            N_L = - yL / yp\n",
    "\n",
    "        elif \"HERA_CC\" in experiment_name:\n",
    "            N_2 = 1 / 4 * yp\n",
    "            N_L = - N_2 * yL / yp\n",
    "\n",
    "    if \"CHORUS_CC\" in experiment_name:\n",
    "        yL = np.power(y, 2)\n",
    "        Gf = 1.1663787e-05\n",
    "        Mh = 0.938\n",
    "        MW2 = 80.398 ** 2\n",
    "        yp = 1 + np.power(1 - y, 2) - 2 * np.power(x * y * Mh, 2) / Q2\n",
    "        N_2 = Gf**2 * Mh * yp / ( 2 * np.pi * np.power( 1 + Q2 / MW2, 2) )\n",
    "        N_L = - N_2 * yL / yp\n",
    "\n",
    "    return N_2, N_L"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "76acc02c",
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.options.mode.chained_assignment = None\n",
    "# compute the delta of the theory prediction\n",
    "if version == 1:\n",
    "    preds_ht[\"higher twist\"] = 0\n",
    "    preds_ht.loc[['DIS NC', 'DIS CC'],'higher twist'] = ht_coeff * (\n",
    "        preds_ht.loc[['DIS NC', 'DIS CC'], 'theory_central'] / q2vals_DIS/ (1 - xvals_DIS)\n",
    "    )\n",
    "elif version == 2:\n",
    "    preds_ht[\"higher twist\"] = 0\n",
    "    preds_ht.loc[['DIS NC', 'DIS CC'],'higher twist'] = preds_ht.loc[['DIS NC', 'DIS CC'], 'theory_central'] * ( \n",
    "         ht_coeff_1 + ht_coeff_2 * xvals_DIS / (1 - xvals_DIS)\n",
    "    ) / q2vals_DIS\n",
    "elif version == 3:\n",
    "    preds_ht[\"higher twist (±,0)\"] = 0\n",
    "    preds_ht[\"higher twist (0,±)\"] = 0\n",
    "    # Compute beta for shift (±,0)\n",
    "    preds_ht.loc[['DIS NC', 'DIS CC'],'higher twist (±,0)'] = preds_ht.loc[['DIS NC', 'DIS CC'], 'theory_central'] * (\n",
    "        ht_coeff_1 / q2vals_DIS\n",
    "    )\n",
    "    preds_ht.loc[['DIS NC', 'DIS CC'],'higher twist (0,±)'] = preds_ht.loc[['DIS NC', 'DIS CC'], 'theory_central'] * (\n",
    "        ht_coeff_2 * xvals_DIS / (1 - xvals_DIS) / q2vals_DIS\n",
    "        )\n",
    "elif version == 4:\n",
    "    # ABMP parametrisation\n",
    "    x = [0.0, 0.1, 0.3, 0.5, 0.7, 0.9, 1]\n",
    "    y_2 = [0.023, -0.032, -0.005, 0.025, 0.051, 0.003, 0.0]\n",
    "    y_T = [-0.319, -0.134, -0.052, 0.071, 0.030, 0.003, 0.0]\n",
    "    H_2 = sp.interpolate.CubicSpline(x, y_2)\n",
    "    H_2 = np.vectorize(H_2)\n",
    "    H_T = sp.interpolate.CubicSpline(x, y_T)\n",
    "    def H_L(x):\n",
    "        return (H_2(x) - np.power(x, 0.05) * H_T(x))\n",
    "    H_L = np.vectorize(H_L)\n",
    "\n",
    "    included_proc = [\"DIS NC\"]\n",
    "    excluded_exp = {\"DIS NC\" : [\"NMC_NC_NOTFIXED_DW_EM-F2\"]}\n",
    "    included_exp = {}\n",
    "    for proc in included_proc:\n",
    "        aux = []\n",
    "        for exp in process_info.namelist[proc]:\n",
    "            if exp not in excluded_exp[proc]:\n",
    "                aux.append(exp)\n",
    "        included_exp[proc] = aux\n",
    "\n",
    "    preds_ht.loc[['DIS NC', 'DIS CC'], 'x'] = xvals_DIS\n",
    "    preds_ht.loc[['DIS NC', 'DIS CC'], 'q2'] = q2vals_DIS\n",
    "    preds_ht.loc[['DIS NC', 'DIS CC'], 'y'] = yvals_DIS\n",
    "    \n",
    "    preds_ht[\"higher twist (±,0)\"] = 0\n",
    "    preds_ht[\"higher twist (0,±)\"] = 0\n",
    "\n",
    "    ht_array_1 = np.array([])\n",
    "    ht_array_2 = np.array([])\n",
    "\n",
    "    for proc in process_info.namelist.keys():\n",
    "        for exp in process_info.namelist[proc]:\n",
    "            if proc in included_proc and exp in included_exp[proc]:\n",
    "                x  = np.array(preds_ht.xs(exp, level=1, drop_level=False).loc[:,\"x\"])\n",
    "                Q2 = np.array(preds_ht.xs(exp, level=1, drop_level=False).loc[:,\"q2\"])\n",
    "                y  = np.array(preds_ht.xs(exp, level=1, drop_level=False).loc[:,\"y\"])\n",
    "                N = np.array([])\n",
    "\n",
    "                if \"SIGMA\" in exp:\n",
    "                    N_2, N_L = compute_normalisation_by_experiment(exp, x, y, Q2)\n",
    "                elif \"F2\" in exp:\n",
    "                    N_2 = np.ones(shape=x.shape)\n",
    "                    N_L = np.zeros(shape=x.shape)\n",
    "                else:\n",
    "                    raise ValueError(f\"The normalisation for the observable is not known.\")\n",
    "\n",
    "                #preds_ht.xs(exp, level=1, drop_level=False).loc[:,\"higher twist (±,0)\"] = N_2 * ht_coeff_1 * H_2(x) / Q2\n",
    "                #preds_ht.xs(exp, level=1, drop_level=False).loc[:,\"higher twist (0,±)\"] = N_L * ht_coeff_2 * H_L(x) / Q2\n",
    "                ht_array_1 = np.append(ht_array_1, np.array(N_2 * ht_coeff_1 * H_2(x) / Q2))\n",
    "                ht_array_2 = np.append(ht_array_2, np.array(N_L * ht_coeff_2 * H_L(x) / Q2))\n",
    "            else:\n",
    "                ht_array_1 = np.append(ht_array_1, np.zeros(preds_ht.xs(exp, level=1, drop_level=False).shape[0]))\n",
    "                ht_array_2 = np.append(ht_array_2, np.zeros(preds_ht.xs(exp, level=1, drop_level=False).shape[0]))\n",
    "\n",
    "preds_ht[\"higher twist (±,0)\"] = ht_array_1\n",
    "preds_ht[\"higher twist (0,±)\"] = ht_array_2\n",
    "delta_pred = []\n",
    "\n",
    "if version == 1 or version == 2:\n",
    "    delta_pred.append(preds_ht['higher twist'])\n",
    "elif version == 3 or version == 4:\n",
    "    delta_pred.append(preds_ht['higher twist (±,0)'])\n",
    "    delta_pred.append(preds_ht['higher twist (0,±)'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "800cd4de",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Theory covariance matrix\n",
    "S = np.zeros((delta_pred[0].size, delta_pred[0].size))\n",
    "for delta in delta_pred:\n",
    "    S += np.outer(delta, delta)\n",
    "\n",
    "S = pd.DataFrame(S, index=delta_pred[0].index, columns=delta_pred[0].index)\n",
    "\n",
    "# Experimental covariance matrix\n",
    "C = API.groups_covmat_no_table(**common_dict)\n",
    "\n",
    "# Ensure that S anc C are ordered in the same way (in practice they already are)\n",
    "S = S.reindex(C.index).T.reindex(C.index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "d896cf55",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the central value of the pseudodata\n",
    "# this is needed to compute the distance between prediction and data\n",
    "pseudodata = API.read_pdf_pseudodata(**common_dict)\n",
    "dat_central = np.mean(\n",
    "    [i.pseudodata.reindex(preds.index.to_list()).to_numpy().flatten() for i in pseudodata],\n",
    "    axis=0,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "d373edbc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute delta_T_tilde (Eq. 3.37) and P_tilde (Eq. 3.38) of arXiv:2105.05114\n",
    "\n",
    "# The factors 1/sqrt(2) are to normalize for the fact that beta provides information about\n",
    "# theoretical uncertainties along two directions\n",
    "# CHECK THIS PART\n",
    "# b_tilde SHOULD BE INDEPENDENT OF THE PRIOR THAT WE USE TO MODEL HT CORRECTIONS.\n",
    "if version == 1:\n",
    "    central_ht_coeffs = [0] # central prediction for ht_coeff\n",
    "    beta_tilde = [[ht_coeff]]\n",
    "\n",
    "elif version == 2:\n",
    "    central_ht_coeffs = [0, 0] # central prediction for ht_coeff_1 and ht_coeff_2\n",
    "    beta_tilde = [[ht_coeff_1, ht_coeff_2]]\n",
    "\n",
    "elif version == 3 or version == 4:\n",
    "    central_ht_coeffs = [0, 0] # central prediction for ht_coeff_1 and ht_coeff_2\n",
    "    beta_tilde = [[ht_coeff_1, 0], [0, ht_coeff_2]]\n",
    "\n",
    "S_tilde = np.zeros((len(beta_tilde[0]), len(beta_tilde[0])))\n",
    "for tilde in beta_tilde:\n",
    "    S_tilde += np.outer(tilde,tilde)\n",
    "\n",
    "beta = delta_pred\n",
    "S_hat = np.zeros((len(beta_tilde[0]),delta_pred[0].size))\n",
    "for b in zip(beta_tilde, beta):\n",
    "    S_hat += np.outer(b[0], b[1])\n",
    "\n",
    "invcov = np.linalg.inv(C + S)\n",
    "\n",
    "delta_T_tilde = -S_hat @ invcov @ (mean_prediction - dat_central)\n",
    "# where are the X_tilde and X_hat terms in P_tilde?\n",
    "# Maybe not present because we don't have correlations between theory parameters\n",
    "P_tilde = S_hat @ invcov @ X @ invcov @ S_hat.T + (S_tilde - S_hat @ invcov @ S_hat.T)\n",
    "preds = central_ht_coeffs + delta_T_tilde\n",
    "uncs = np.sqrt(P_tilde)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "f50ade72",
   "metadata": {},
   "outputs": [],
   "source": [
    "# check if the stored covmat is equal to S we recomputed above\n",
    "fitpath = API.fit(fit=fitname).path\n",
    "try:\n",
    "    stored_covmat = pd.read_csv(\n",
    "        fitpath / \"tables/datacuts_theory_theorycovmatconfig_user_covmat.csv\",\n",
    "        sep=\"\\t\",\n",
    "        encoding=\"utf-8\",\n",
    "        index_col=2,\n",
    "        header=3,\n",
    "        skip_blank_lines=False,\n",
    "    )\n",
    "except FileNotFoundError:\n",
    "    stored_covmat = pd.read_csv(\n",
    "        fitpath / \"tables/datacuts_theory_theorycovmatconfig_theory_covmat_custom.csv\",\n",
    "        index_col=[0, 1, 2],\n",
    "        header=[0, 1, 2],\n",
    "        sep=\"\\t|,\",\n",
    "        engine=\"python\",\n",
    "    ).fillna(0)\n",
    "    storedcovmat_index = pd.MultiIndex.from_tuples(\n",
    "        [(aa, bb, np.int64(cc)) for aa, bb, cc in stored_covmat.index],\n",
    "        names=[\"group\", \"dataset\", \"id\"],\n",
    "    )\n",
    "    stored_covmat = pd.DataFrame(\n",
    "        stored_covmat.values, index=storedcovmat_index, columns=storedcovmat_index\n",
    "    )\n",
    "    stored_covmat = stored_covmat.reindex(S.index).T.reindex(S.index)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "ae6f7cb4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.78536045, 0.33293247])"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "preds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "72220133",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.09611684, 0.06289351],\n",
       "       [0.06289351, 0.13647068]])"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "uncs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "523f127f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# print the final result\n",
    "if np.allclose(S, stored_covmat):\n",
    "    if version == 1:\n",
    "        print(\n",
    "            f\"Prediction for ht_coeff: {preds[0]:.5f} ± {uncs[0,0]:.5f} \\n\"\n",
    "            f\"ht_coeff : {ht_coeff}\"\n",
    "            )\n",
    "    elif version == 2:\n",
    "        print(\n",
    "            f\"Prediction for \\n ht_coeff_1: {preds[0]:.5f} ± {uncs[0,0]:.5f} \\n ht_coeff_2: {preds[1]:.5f} ± {uncs[1,1]:.5f}\"\n",
    "            )\n",
    "else:\n",
    "    print(\"Reconstructed theory covmat, S, is not the same as the stored covmat!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ec79a12",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.colors as mcolors\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(9, 6))\n",
    "ax.scatter(\n",
    "    xvals,\n",
    "    q2vals,\n",
    "    marker=\".\",\n",
    "    c=stored_covmat.to_numpy().diagonal(),\n",
    "    cmap=\"viridis\",\n",
    "    norm=mcolors.LogNorm(),\n",
    ")\n",
    "ax.set_xscale(\"log\")\n",
    "ax.set_yscale(\"log\")\n",
    "ax.set_xlabel(r\"$x$\")\n",
    "ax.set_ylabel(r\"$Q^2$\")\n",
    "\n",
    "filterlist = []\n",
    "q2min = 5\n",
    "w2min = 20\n",
    "for i,(x,q2) in enumerate(zip(xvals, q2vals)):\n",
    "    w2 = q2*(1-x)/x\n",
    "    if q2 < q2min:\n",
    "        filterlist.append(i)\n",
    "    elif w2 < w2min:\n",
    "        filterlist.append(i)\n",
    "\n",
    "ax.scatter(\n",
    "    xvals[filterlist],\n",
    "    q2vals[filterlist],\n",
    "    marker=\".\",\n",
    "    color=\"black\"\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63e11d00",
   "metadata": {},
   "outputs": [],
   "source": [
    "API.plot_xq2(\n",
    "    fit=fitname,\n",
    "    dataset_inputs={\"from_\": \"fit\"},\n",
    "    use_cuts=\"fromfit\",\n",
    "    display_cuts=False,\n",
    "    marker_by=\"dataset\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa074b58",
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.core.display import display\n",
    "pdfs = [\n",
    "    \"240109-02-rs-ht-tcm-disonly\",\n",
    "    \"240109-03-rs-ht-tcm-disonly\"\n",
    "]\n",
    "%matplotlib widget\n",
    "flavours = ['g', 'u', 'd', 'ubar' ]\n",
    "figs = API.plot_pdfs(pdfs=pdfs, Q=1.65, flavours=flavours)\n",
    "for fig, fl in figs:\n",
    "    fig.tight_layout()\n",
    "    display(fig)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab072385",
   "metadata": {},
   "source": [
    "# Comparison to [`arxiv:1701.05838`](https://arxiv.org/pdf/1701.05838.pdf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aaab1587",
   "metadata": {},
   "outputs": [],
   "source": [
    "import scipy as sp\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7535419",
   "metadata": {},
   "source": [
    "`ABMP16` makes use a cubic spline parametrisation of the HT term with 7 knots. Here, I define the positions of the knots in the x-grid and the respective value. This makes the cubic spline unique."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91d984dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "x = [0.0, 0.1, 0.3, 0.5, 0.7, 0.9, 1]\n",
    "y = [0.023, -0.032, -0.005, 0.025, 0.051, 0.003, 0.0]\n",
    "sigma_y = [0.019, 0.013, 0.009, 0.006, 0.005, 0.004, 0]\n",
    "yp = [x1 + x2 for x1,x2 in zip(y, sigma_y)]\n",
    "ym = [x1 - x2 for x1,x2 in zip(y, sigma_y)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "252ddc9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "cs = sp.interpolate.CubicSpline(x, y)\n",
    "csp = sp.interpolate.CubicSpline(x, yp)\n",
    "csm = sp.interpolate.CubicSpline(x, ym)\n",
    "cs_color = \"sandybrown\"\n",
    "cs_label = \"± sigma\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71493d9c",
   "metadata": {},
   "source": [
    "I compute the predictions for the HT term with the posteriors obtained in the previous analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01fdedcb",
   "metadata": {},
   "outputs": [],
   "source": [
    "h1_posterior = 0.11932\n",
    "h1_sigma = 0.00983\n",
    "h2_posterior = 0.93714\n",
    "h2_sigma = 0.04494\n",
    "h_corr = -0.00031\n",
    "preds_ht_posterior = {}\n",
    "preds_ht_posterior_sigma = {}\n",
    "x_for_plot = {}\n",
    "\n",
    "keys = [\"SLAC_NC_NOTFIXED_P_DW_EM-F2\",\n",
    "        #\"SLAC_NC_NOTFIXED_D_DW_EM-F2\",\n",
    "        #\"BCDMS_NC_NOTFIXED_P_DW_EM-F2\",\n",
    "        #\"BCDMS_NC_NOTFIXED_D_DW_EM-F2\"\n",
    "]\n",
    "\n",
    "preds_ht[\"q2\"] = 0\n",
    "preds_ht.loc[['DIS NC', 'DIS CC'], 'x'] = xvals_DIS\n",
    "preds_ht.loc[['DIS NC', 'DIS CC'], 'q2'] = q2vals_DIS\n",
    "\n",
    "for key in keys:\n",
    "        theory = preds_ht.xs(key, level=1, drop_level=False).loc[:,\"theory_central\"]\n",
    "        x_ht = preds_ht.xs(key, level=1, drop_level=False).loc[:,\"x\"]\n",
    "        q2 = preds_ht.xs(key, level=1, drop_level=False).loc[:,\"q2\"]\n",
    "        preds_ht_posterior[key] = ( h1_posterior + h2_posterior * x_ht / (1 - x_ht) ) / q2\n",
    "        dh1 = theory\n",
    "        dh2 = theory * x_ht / (1 - x_ht)\n",
    "        preds_ht_posterior_sigma[key] = np.sqrt(np.power(dh1 * h1_sigma, 2) + np.power(dh2 * h2_sigma, 2) +  2 * dh1 * dh2 * h_corr )\n",
    "        x_for_plot[key] = x_ht\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb896a7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "xv = np.logspace(-5, -0.0001, 100)\n",
    "legends = []\n",
    "legend_name = [\"ABMP16\", \"knots\"]\n",
    "fig, ax = plt.subplots(figsize=(12.5, 8))\n",
    "knots = ax.plot(x, y, 'o', label='data')\n",
    "pl = ax.plot(xv, cs(xv), ls = \"-\", lw = 1, color=cs_color)\n",
    "pl_lg= ax.fill(np.NaN, np.NaN, color = cs_color, alpha = 0.3) # Necessary for fancy legend\n",
    "legends.append((pl[0], pl_lg[0]))\n",
    "legends.append(knots[0])\n",
    "ax.fill_between(xv, csp(xv), csm(xv), color = cs_color, alpha = 0.3)\n",
    "ax.set_xscale(\"log\")\n",
    "ax.set_xlabel(f'$x$')\n",
    "ax.set_ylabel(f'$H(x)$')\n",
    "\n",
    "# Plot predictions per data set\n",
    "for set in preds_ht_posterior.keys():\n",
    "    pt = ax.errorbar(x_for_plot[set], preds_ht_posterior[set],\n",
    "                     yerr=preds_ht_posterior_sigma[set],\n",
    "                     marker='o',linestyle='None', markersize=5, alpha=0.2)\n",
    "    #pt_th = ax.plot(x_for_plot[set], theory_for_plot[set], marker='o',linestyle='None', markersize=5)\n",
    "    legends.append(pt[0])\n",
    "    legend_name.append(set)\n",
    "\n",
    "\n",
    "fig.legend(legends, legend_name, loc=[0.1,0.08], fontsize=10)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96bd1a64",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nnpdf",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  },
  "vscode": {
   "interpreter": {
    "hash": "589e3134e9d89160e5ace28972e8dc0b682f48816407b59cbfdad217f6fb745b"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
